{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d469f07",
   "metadata": {},
   "source": [
    "# Advanced AI: Transformers for NLP Using Large Language Models\n",
    "\n",
    "**Instructor:** Jonathan Fernandes\n",
    "\n",
    "Transformers have quickly become the go-to architecture for natural language processing (NLP). As a result, knowing how to use them is now a business-critical skill in your AI toolbox. In this course, instructor Jonathan Fernandes walks you through many of the key large language models developed since GPT-3. He presents a high-level overview of GLaM, Megatron-Turing NLG, Gopher, Chinchilla, PaLM, OPT, and BLOOM, relaying some of the most important insights from each model.\n",
    "\n",
    "Get a high-level overview of large language models, where and how they are used in production, and why they are so important to NLP. Additionally, discover the basics of transfer learning and transformer training to optimize your AI models as you go. By the end of this course, you’ll be up to speed with what’s happened since OpenAI first released GPT-3 as well as the key contributions of each of these large language models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19f6184",
   "metadata": {},
   "source": [
    "## 1. Transformers in NLP\n",
    "### What are large language models?\n",
    "- BERT and GPT-3 are examples of large language models\n",
    "- **Large language model architecture is based on transformer architecture.**\n",
    "- Transformers and large language models were proposed by a team of Google researchers in 2017 in a paper entitled: \"**Attention Is All You Need**,\" which has become a turning point in NLP.\n",
    "- Large language models have millions and often billions of parameters and are trained on enormous datasets\n",
    "- GPT-3 was released in May of 2020\n",
    "\n",
    "<img src='img/1.png' width=\"800\" height=\"400\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa08f5a3",
   "metadata": {},
   "source": [
    "- Models released by Google research include: **GLaM**, **PaLM**\n",
    "- Models released by DeepMind include: **Gopher**, **Chinchilla**\n",
    "- Released by Microsoft and Nvidia: Megatron-Turing NLG or **MT-NLG**\n",
    "- Released by Meta AI: **OPT** $\\Rightarrow$ makes large language models available to researchers outside of big tech\n",
    "- Released by Hugging Face: **BLOOM** $\\Rightarrow$ makes large language models available to researchers outside of big tech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840ce9d5",
   "metadata": {},
   "source": [
    "### Transformers in Production\n",
    "\n",
    "#### BERT\n",
    "- In 2019, Google started using BERT as part of search \n",
    "    - $\\Rightarrow$ Now, when entering something into Google search, you can enter something more \"English sounding\"\n",
    "    - For example: instead of \"curling objective\" $\\Rightarrow$ \"what's the main objective of curling?\"\n",
    "    - Another example:\n",
    "        - In the past, if you did a Google search using the phrase \"Can you get medicine for someone pharmacy,\" it would **not** have picked up on the fact that \"for someone\" was a really important part of a query $\\Rightarrow$ But now, it will pick up on the fact that you're looking for another person to pick up the medicine\n",
    "- **BERT**: **B**idirectional **E**ncoder **R**epresentations from **T**ransformers \n",
    "- BERT was fed the Wikipedia and the BookCorpus data as input\n",
    "- One of the first large language models developed by the Google research team\n",
    "- The quality of Google search has improved significantly using BERT.\n",
    "\n",
    "### Transformers: History\n",
    "- The models based on the original transformer paper from 2017 have evolved over the years.\n",
    "- One of the challenges with training large language models in 2017 was that you needed labeled data.\n",
    "- The ULMFiT model proposed by Jeremy Howard and Sebastian Ruda provided a framework where you didn't need labeled data, and that meant **large corpus of texts, such as Wikipedia, could now be used to train models.**\n",
    "- In June of 2018, GPT or **G**enerative **P**re-**T**rained Model, developed by Open AI, was the first pre-trained transformer model.\n",
    "- When Open AI released a bigger and better version of GPT (GPT-2) in Feb 2019, it made headlines because the team didn't want to release the details of the model due to **ethical concerns.**\n",
    "- Meta's BART and Google's T5 are both large pre-trained models using the same architecture as the original transformer\n",
    "- Hugging Face released DistilBERT, which is a smaller, faster, and lighter version of BERT: DistilBERT had 95% the performace of BERT and reduced the size of the BERT model by 40%\n",
    "- In May 2020 Open AI released GPT-3, which is excellent at generating high-quality English sentences.\n",
    "    - Although Open AI provided a lot of details in their GPT-3 paper, they didn't reveal the dataset they used or thier model weights\n",
    "    \n",
    "<img src='img/2.png' width=\"800\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "<img src='img/3.png' width=\"800\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "**Note** that in the graph above, the y-axis is on a log scale, and so the growth is not linear but exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3ae468",
   "metadata": {},
   "source": [
    "## 2. Training Transformers and Their Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff99e6c",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "- Transfer learning is made up of 2 components:\n",
    "    - **Pre-training** $\\Rightarrow$ Extremely resource-heavy\n",
    "    - **Fine-tuning** $\\Rightarrow$ Involves training our model with labeled data\n",
    "    \n",
    "<img src='img/4.png' width=\"800\" height=\"400\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34acdae5",
   "metadata": {},
   "source": [
    "#### Pre-training Tasks: BERT (Google)\n",
    "- **Masked language modeling:** Fed Wikipedia and BookCorpus data as input and words were randomly masked out\n",
    "- BERT then had to predict what the most likely candidates were for these masked words\n",
    "- With **Next sentence prediction**, it had to predict whether one sentence followed the other.\n",
    "    - 50% of the time one sentence did follow the other and these were labeled as `isNext`\n",
    "    - 50% of the time a random other sentence from the corpus was used, and these were labeled as `notNext`\n",
    "- According to BERT's documentation, **1,500 words is approximately equivalent to 2,400 tokens.**\n",
    "    - So this means **one word is approximately 1.4 tokens.**\n",
    "    - **A novel of 100,000 words is approximately 140,000 tokens.**\n",
    "    \n",
    "#### RoBERTa (Facebook)\n",
    "- Trained in one day\n",
    "- 2 trillion tokens\n",
    "- Also used were Wikipedia, BookCorpus, as well as the Common Crawl news dataset, OpenWebText, and the Common Crawl stories:\n",
    "    - **Common Crawl** is a raw webpage dataset from years of web crawling \n",
    "    - **OpenWebText** is a dataset created by scraping URLs from REddit with a score of three (this is a proxy for the quality of the data response)\n",
    "    \n",
    "#### GPT-3 (Open AI)\n",
    "- 34 days training days\n",
    "- Used 10,000 V100 GPUs\n",
    "- 300B training tokens\n",
    "- Primarily an Azure infrastructure\n",
    "- Used Wikipedia, CommonCrawl, WebText2, Books1, Books2\n",
    "\n",
    "#### Benefits of Transfer Learning\n",
    "- Faster development \n",
    "    - For BERT, the author suggest two to four epochs of training\n",
    "    - Much better than the thousands of hours of pre-training time\n",
    "- Less data to fine-tune\n",
    "- Excellent results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dba68c3",
   "metadata": {},
   "source": [
    "### Transformer Architecture\n",
    "\n",
    "#### Encoder-Decoder Models\n",
    "- From the **\"Attention Is All You Need\"** paper:\n",
    "\n",
    "<img src='img/5.png' width=\"500\" height=\"250\" align=\"center\"/>\n",
    "\n",
    "- The left-hand side is known as an **encoder** and the right-hand side is known as a **decoder**. \n",
    "- We feed in the English sentence, such as \"I like NLP,\" and the decoder can act as a transformer of the sentence from English to German:\n",
    "\n",
    "<img src='img/6.png' width=\"500\" height=\"250\" align=\"center\"/>\n",
    "\n",
    "- However, the transformer is not made up of a single encoder, but rather six encoders. \n",
    "- Each of these parts can be used independently depending on the task.\n",
    "- Encoder-Decoder models are good at generative tasks such as translation or summarization\n",
    "- Examples of encoder-decoder models are:\n",
    "    - BART (Facebook)\n",
    "    - T5 (Google)\n",
    "\n",
    "<img src='img/7.png' width=\"500\" height=\"250\" align=\"center\"/>\n",
    "\n",
    "<img src='img/8.png' width=\"500\" height=\"250\" align=\"center\"/>\n",
    "\n",
    "#### Encoder-only Models\n",
    "\n",
    "- Encoder-only models are good for tasks that require understanding of the input, such as:\n",
    "    - Sentence classification\n",
    "    - Named entity recognition (NER)\n",
    "- Examples include the family of BERT models:\n",
    "    - BERT\n",
    "    - RoBERTa\n",
    "    - DistilBERT\n",
    "    \n",
    "<img src='img/9.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "#### Decoder-only Models\n",
    "- Good for generative tasks such as text generation\n",
    "- Examples include: \n",
    "    - GPT\n",
    "    - GPT-2\n",
    "    - GPT-3\n",
    "\n",
    "<img src='img/10.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "    \n",
    "    \n",
    "#### $\\star$ $\\star$ $\\star$ In summary, transformers are made up of encoders and decoders, and the tasks we can perform will depend on whether we use either or both components $\\star$ $\\star$ $\\star$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd06280c",
   "metadata": {},
   "source": [
    "### Self-Attention\n",
    "- One of the key ingredients to transformers is **self-attention.**\n",
    "- In the following example (\"The monkey ate that banana because it was too hungry\"), how is the model able to determine that the \"it\" corresponds to the monkey, and not the banana?\n",
    "    - It does this by using a mechanism called **self-attention**, that incorporates the embeddings for all the other words in the sentence.\n",
    "    - So, when processing the word \"it,\" self-attention will take a weighted average of the embeddings of the other context words\n",
    "    - In the example below, the darker the shade, the more weight that word is given (and every word is given some weight):\n",
    "\n",
    "<img src='img/11.png' width=\"500\" height=\"250\" align=\"center\"/>\n",
    "\n",
    "- So, what's going on under the hood?\n",
    "    - As part of the self-attention mechanism, the authors of the original transformer take the word embeddings and project it into three vector spaces, which they call **Q (query)**, **K (key)**, and **V (value)**.\n",
    "    - Projecting word embeddings into new vector spaces is a tool that mathematicians use to get different representations of the word embeddings\n",
    "    - In order to calculate the attention weights, we'll take in as input the query, key, and value vectors.\n",
    "    - We then calculate the **score** of each word to determine how much focus to place on other words in the sentence.\n",
    "    - We want to try to figure out how the query and the key vectors relate to each other. \n",
    "        - This is done by taking the dot product of the query vector and the key vector.\n",
    "        - Queries and keys that are similar will have a large dot product\n",
    "        - Queries and keys that don't share much in common will have little to no overlap.\n",
    "        - In the equation below:\n",
    "            - T means that we're performing a **transpose** operation on the vector K\n",
    "            - **n** is the dimension of these vectors; we divide by the square root of n to scale the dot product attention, and so reduce its size \n",
    "            - we now have the logits and can convert this to probabilities by using the softmax function \n",
    "            - We then multiply each value vector by the softmax score\n",
    "            - We can then sum up the weighted value vectors, and this produces the self-attention calculation for a word.\n",
    "            - $\\star$ **This process takes place for every single word in the sentence.** $\\star$\n",
    "            - Self-attention allows us to apply a different weight to words in a sentence.\n",
    "\n",
    "<img src='img/12.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41ac6fc",
   "metadata": {},
   "source": [
    "### Multi-head Attention and Feed Forward Network\n",
    "- Above we looked at how self-attention can help us provide context for a word for the sentence \"the monkey ate that banana because it was too hungry,\" but what if we could get multiple instances of the self-attention mechanism, so that each can perform a different task?\n",
    "    - One could make a link between nouns and adjectives\n",
    "    - Another could connect pronouns to their subjects \n",
    "    - etc.\n",
    "- This is the idea behind **multi-headed attention.**\n",
    "- What's particularly impressive is we don't create these relations in the model; they're fully learned from the data.\n",
    "- BERT has 12 such heads, and each multi-head attention block gets three inputs:\n",
    "    - the query\n",
    "    - the key\n",
    "    - the value\n",
    "    \n",
    "<img src='img/13.png' width=\"800\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "- These (12 heads of BERT) are put through linear or dense layers before the multi-head attention function, as shown below.\n",
    "- The query, key, and value are passed through separate, fully-connected, linear layers for each attention head.\n",
    "- This model can jointly attend to information from different representations and at different positions\n",
    "\n",
    "<img src='img/14.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "- By having 12 self-attention heads, the BERT model is able to focus on several tasks at once, thus allowing it to make richer connections between words\n",
    "- Some of the larger language models have significantly more heads\n",
    "    - For example: GPT-3 has 96 such heads\n",
    "- The key takeaway from this section is that multi-head attention allows us to make richer connections between words, and none of these connections are created, but rather they're all learned by the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82097aa7",
   "metadata": {},
   "source": [
    "## 3. Large Language Models\n",
    "### GPT-3\n",
    "- GPT-3 is probably the most well-known large language model\n",
    "- **GPT**: \n",
    "    - **G**enerative: Predicts a future token, given past tokens\n",
    "    - **P**re-trained: Trained on a large corpus of data\n",
    "    - **T**ransformer: *Decoder* portion of the transformer architecture\n",
    "- **GPT-3's objective:** \n",
    "    - **Goal: Given the preceding token, predict the next token.**\n",
    "       - Causal language model\n",
    "       - Autoregressive\n",
    "- **Data trained on:**\n",
    "    - **English Wikipedia**\n",
    "    - **Common Crawl** $\\Rightarrow$ raw webpage data\n",
    "    - **WebText2** $\\Rightarrow$ dataset created by scraping URLs from Reddit (with a score of 3, used as proxy for quality)\n",
    "    - **Books1** $\\Rightarrow$ collection of novels by unpublished authors\n",
    "    - **Books2** $\\Rightarrow$ collection of novels by unpublished authors\n",
    "- **Task trained on:**\n",
    "    - Causal language modeling: predict the next word in a text\n",
    "    - We can train the model in a self-supervised way and we don't have to annotate our datasets\n",
    "    - We can then take all these humungous datasets and use them to train our model\n",
    "    - Additionally, we want to use some decoding algorithms, such as [**beam search**](https://en.wikipedia.org/wiki/Beam_search) to give us a balance of coherent language and diversity so we don't get sentences repeated.\n",
    "\n",
    "<img src='img/15.png' width=\"500\" height=\"250\" align=\"center\"/>\n",
    "\n",
    "- For a couple of years, researches have focused on getting a large corpus of data and training a language model\n",
    "- To use a language model for a specific task (for example: sentiment analysis), you'd have to:\n",
    "    - Give it hundreds of examples of sentences\n",
    "    - Sentences labeled as positive or negative\n",
    "    - Train the model on these sentences and labels\n",
    "    - Model will produce good results\n",
    "    \n",
    "#### What if we could create a language model that if we give it a new task and a couple of examples with the expected output, that it would be able to perform well on these tasks: GPT-3 does just this.\n",
    "\n",
    "- **Prompt:**\n",
    "    - Way to interact with models\n",
    "    - Zero-shot learning $\\Rightarrow$ given task with no examples\n",
    "    - One-shot learning $\\Rightarrow$ given task with one example and expected output\n",
    "    - Few-shot learning $\\Rightarrow$ given task with a couple of examples and expected output\n",
    "    \n",
    "<img src='img/16.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "- **In summary:** GPT-3 provides an easy way to interact with models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3672a74e",
   "metadata": {},
   "source": [
    "### GPT-3 Use Cases\n",
    "- Open AI provides access to GPT-3 via an API: [https://beta.openai.com/playground](https://beta.openai.com/playground)\n",
    "\n",
    "#### Classification\n",
    "- In the following example, we enter the key Fedex with an empty value and the transformer correctly fills in the answer (highlighted in green):\n",
    "\n",
    "<img src='img/17.png' width=\"500\" height=\"250\" align=\"center\"/>\n",
    "\n",
    "- If you look up the notes for Open AI's model training data, this training cuts off in 2021, but Meta and technology seems to be a reasonable answer (below).\n",
    "\n",
    "<img src='img/18.png' width=\"500\" height=\"250\" align=\"center\"/>\n",
    "\n",
    "#### Summarize text for a 2nd grader\n",
    "- We use the first few paragraphs of the Wikipedia entry for GPT-3:\n",
    "\n",
    "<img src='img/19.png' width=\"800\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "<img src='img/20.png' width=\"800\" height=\"400\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357f3ebf",
   "metadata": {},
   "source": [
    "#### Ad from product description\n",
    "\n",
    "<img src='img/21.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "<img src='img/22.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "<img src='img/23.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647c736e",
   "metadata": {},
   "source": [
    "#### Study notes\n",
    "\n",
    "<img src='img/24.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b03ed2d",
   "metadata": {},
   "source": [
    "## Challenges and Shortcomings of GPT-3\n",
    "#### 1. Bias\n",
    "- GPT-3 was trained on data that is biased\n",
    "- Human language and text naturally reflect bias\n",
    "- GPT-3 trained on data deemed interesting on Reddit via upvotes from other users\n",
    "\n",
    "#### 2. Environmental impact\n",
    "- Carbon emissions study of large language models was conducted by Google and Berkeley in 2021 and found that **training GPT-3 would've resulted in energy consumption of almost 1,300 MMWh (megawatt hours) and the release of 550 tons of $CO_2$**\n",
    "\n",
    "\n",
    "#### It's worth noting that some of the large language models that followed GPT-3 tried to optimize/address some of these challenges (bias and environmental impact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e04c9a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd300501",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b836bfed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67efd956",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bdeb68e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "555b2ef3",
   "metadata": {},
   "source": [
    "<img src='img/x.png' width=\"800\" height=\"400\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d9a473",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
