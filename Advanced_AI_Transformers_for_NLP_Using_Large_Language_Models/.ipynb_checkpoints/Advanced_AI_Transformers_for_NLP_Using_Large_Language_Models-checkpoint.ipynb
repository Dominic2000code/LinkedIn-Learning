{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d469f07",
   "metadata": {},
   "source": [
    "# Advanced AI: Transformers for NLP Using Large Language Models\n",
    "\n",
    "**Instructor:** Jonathan Fernandes\n",
    "\n",
    "Transformers have quickly become the go-to architecture for natural language processing (NLP). As a result, knowing how to use them is now a business-critical skill in your AI toolbox. In this course, instructor Jonathan Fernandes walks you through many of the key large language models developed since GPT-3. He presents a high-level overview of GLaM, Megatron-Turing NLG, Gopher, Chinchilla, PaLM, OPT, and BLOOM, relaying some of the most important insights from each model.\n",
    "\n",
    "Get a high-level overview of large language models, where and how they are used in production, and why they are so important to NLP. Additionally, discover the basics of transfer learning and transformer training to optimize your AI models as you go. By the end of this course, you’ll be up to speed with what’s happened since OpenAI first released GPT-3 as well as the key contributions of each of these large language models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19f6184",
   "metadata": {},
   "source": [
    "## 1. Transformers in NLP\n",
    "### What are large language models?\n",
    "- BERT and GPT-3 are examples of large language models\n",
    "- **Large language model architecture is based on transformer architecture.**\n",
    "- Transformers and large language models were proposed by a team of Google researchers in 2017 in a paper entitled: \"**Attention Is All You Need**,\" which has become a turning point in NLP.\n",
    "- Large language models have millions and often billions of parameters and are trained on enormous datasets\n",
    "- GPT-3 was released in May of 2020\n",
    "\n",
    "<img src='img/1.png' width=\"800\" height=\"400\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa08f5a3",
   "metadata": {},
   "source": [
    "- Models released by Google research include: **GLaM**, **PaLM**\n",
    "- Models released by DeepMind include: **Gopher**, **Chinchilla**\n",
    "- Released by Microsoft and Nvidia: Megatron-Turing NLG or **MT-NLG**\n",
    "- Released by Meta AI: **OPT** $\\Rightarrow$ makes large language models available to researchers outside of big tech\n",
    "- Released by Hugging Face: **BLOOM** $\\Rightarrow$ makes large language models available to researchers outside of big tech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840ce9d5",
   "metadata": {},
   "source": [
    "### Transformers in Production\n",
    "\n",
    "#### BERT\n",
    "- In 2019, Google started using BERT as part of search \n",
    "    - $\\Rightarrow$ Now, when entering something into Google search, you can enter something more \"English sounding\"\n",
    "    - For example: instead of \"curling objective\" $\\Rightarrow$ \"what's the main objective of curling?\"\n",
    "    - Another example:\n",
    "        - In the past, if you did a Google search using the phrase \"Can you get medicine for someone pharmacy,\" it would **not** have picked up on the fact that \"for someone\" was a really important part of a query $\\Rightarrow$ But now, it will pick up on the fact that you're looking for another person to pick up the medicine\n",
    "- **BERT**: **B**idirectional **E**ncoder **R**epresentations from **T**ransformers \n",
    "- One of the first large language models developed by the Google research team\n",
    "- The quality of Google search has improved significantly using BERT.\n",
    "\n",
    "### Transformers: History\n",
    "- The models based on the original transformer paper from 2017 have evolved over the years.\n",
    "- One of the challenges with training large language models in 2017 was that you needed labeled data.\n",
    "- The ULMFIT odel proposed by Jeremy Howard and Sebastian Ruda provided a framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3ae468",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ff99e6c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3672a74e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc556102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "555b2ef3",
   "metadata": {},
   "source": [
    "<img src='img/x.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792a0bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
