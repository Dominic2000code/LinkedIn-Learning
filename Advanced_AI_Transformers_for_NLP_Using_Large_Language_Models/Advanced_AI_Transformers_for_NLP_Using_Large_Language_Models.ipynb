{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d469f07",
   "metadata": {},
   "source": [
    "# Advanced AI: Transformers for NLP Using Large Language Models\n",
    "\n",
    "**Instructor:** Jonathan Fernandes\n",
    "\n",
    "Transformers have quickly become the go-to architecture for natural language processing (NLP). As a result, knowing how to use them is now a business-critical skill in your AI toolbox. In this course, instructor Jonathan Fernandes walks you through many of the key large language models developed since GPT-3. He presents a high-level overview of GLaM, Megatron-Turing NLG, Gopher, Chinchilla, PaLM, OPT, and BLOOM, relaying some of the most important insights from each model.\n",
    "\n",
    "Get a high-level overview of large language models, where and how they are used in production, and why they are so important to NLP. Additionally, discover the basics of transfer learning and transformer training to optimize your AI models as you go. By the end of this course, you’ll be up to speed with what’s happened since OpenAI first released GPT-3 as well as the key contributions of each of these large language models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19f6184",
   "metadata": {},
   "source": [
    "## 1. Transformers in NLP\n",
    "### What are large language models?\n",
    "- BERT and GPT-3 are examples of large language models\n",
    "- **Large language model architecture is based on transformer architecture.**\n",
    "- Transformers and large language models were proposed by a team of Google researchers in 2017 in a paper entitled: \"**Attention Is All You Need**,\" which has become a turning point in NLP.\n",
    "- Large language models have millions and often billions of parameters and are trained on enormous datasets\n",
    "- GPT-3 was released in May of 2020\n",
    "\n",
    "<img src='img/1.png' width=\"800\" height=\"400\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa08f5a3",
   "metadata": {},
   "source": [
    "- Models released by Google research include: **GLaM**, **PaLM**\n",
    "- Models released by DeepMind include: **Gopher**, **Chinchilla**\n",
    "- Released by Microsoft and Nvidia: Megatron-Turing NLG or **MT-NLG**\n",
    "- Released by Meta AI: **OPT** $\\Rightarrow$ makes large language models available to researchers outside of big tech\n",
    "- Released by Hugging Face: **BLOOM** $\\Rightarrow$ makes large language models available to researchers outside of big tech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840ce9d5",
   "metadata": {},
   "source": [
    "### Transformers in Production\n",
    "\n",
    "#### BERT\n",
    "- In 2019, Google started using BERT as part of search \n",
    "    - $\\Rightarrow$ Now, when entering something into Google search, you can enter something more \"English sounding\"\n",
    "    - For example: instead of \"curling objective\" $\\Rightarrow$ \"what's the main objective of curling?\"\n",
    "    - Another example:\n",
    "        - In the past, if you did a Google search using the phrase \"Can you get medicine for someone pharmacy,\" it would **not** have picked up on the fact that \"for someone\" was a really important part of a query $\\Rightarrow$ But now, it will pick up on the fact that you're looking for another person to pick up the medicine\n",
    "- **BERT**: **B**idirectional **E**ncoder **R**epresentations from **T**ransformers \n",
    "- BERT was fed the Wikipedia and the BookCorpus data as input\n",
    "- One of the first large language models developed by the Google research team\n",
    "- The quality of Google search has improved significantly using BERT.\n",
    "\n",
    "### Transformers: History\n",
    "- The models based on the original transformer paper from 2017 have evolved over the years.\n",
    "- One of the challenges with training large language models in 2017 was that you needed labeled data.\n",
    "- The ULMFiT model proposed by Jeremy Howard and Sebastian Ruda provided a framework where you didn't need labeled data, and that meant **large corpus of texts, such as Wikipedia, could now be used to train models.**\n",
    "- In June of 2018, GPT or **G**enerative **P**re-**T**rained Model, developed by Open AI, was the first pre-trained transformer model.\n",
    "- When Open AI released a bigger and better version of GPT (GPT-2) in Feb 2019, it made headlines because the team didn't want to release the details of the model due to **ethical concerns.**\n",
    "- Meta's BART and Google's T5 are both large pre-trained models using the same architecture as the original transformer\n",
    "- Hugging Face released DistilBERT, which is a smaller, faster, and lighter version of BERT: DistilBERT had 95% the performace of BERT and reduced the size of the BERT model by 40%\n",
    "- In May 2020 Open AI released GPT-3, which is excellent at generating high-quality English sentences.\n",
    "    - Although Open AI provided a lot of details in their GPT-3 paper, they didn't reveal the dataset they used or thier model weights\n",
    "    \n",
    "<img src='img/2.png' width=\"800\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "<img src='img/3.png' width=\"800\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "**Note** that in the graph above, the y-axis is on a log scale, and so the growth is not linear but exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3ae468",
   "metadata": {},
   "source": [
    "## 2. Training Transformers and Their Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff99e6c",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "- Transfer learning is made up of 2 components:\n",
    "    - **Pre-training** $\\Rightarrow$ Extremely resource-heavy\n",
    "    - **Fine-tuning** $\\Rightarrow$ Involves training our model with labeled data\n",
    "    \n",
    "<img src='img/4.png' width=\"800\" height=\"400\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34acdae5",
   "metadata": {},
   "source": [
    "#### Pre-training Tasks: BERT (Google)\n",
    "- **Masked language modeling:** Fed Wikipedia and BookCorpus data as input and words were randomly masked out\n",
    "- BERT then had to predict what the most likely candidates were for these masked words\n",
    "- With **Next sentence prediction**, it had to predict whether one sentence followed the other.\n",
    "    - 50% of the time one sentence did follow the other and these were labeled as `isNext`\n",
    "    - 50% of the time a random other sentence from the corpus was used, and these were labeled as `notNext`\n",
    "- According to BERT's documentation, **1,500 words is approximately equivalent to 2,400 tokens.**\n",
    "    - So this means **one word is approximately 1.4 tokens.**\n",
    "    - **A novel of 100,000 words is approximately 140,000 tokens.**\n",
    "    \n",
    "#### RoBERTa (Facebook)\n",
    "- Trained in one day\n",
    "- 2 trillion tokens\n",
    "- Also used were Wikipedia, BookCorpus, as well as the Common Crawl news dataset, OpenWebText, and the Common Crawl stories:\n",
    "    - **Common Crawl** is a raw webpage dataset from years of web crawling \n",
    "    - **OpenWebText** is a dataset created by scraping URLs from REddit with a score of three (this is a proxy for the quality of the data response)\n",
    "    \n",
    "#### GPT-3 (Open AI)\n",
    "- 34 days training days\n",
    "- Used 10,000 V100 GPUs\n",
    "- 300B training tokens\n",
    "- Primarily an Azure infrastructure\n",
    "- Used Wikipedia, CommonCrawl, WebText2, Books1, Books2\n",
    "\n",
    "#### Benefits of Transfer Learning\n",
    "- Faster development \n",
    "    - For BERT, the author suggest two to four epochs of training\n",
    "    - Much better than the thousands of hours of pre-training time\n",
    "- Less data to fine-tune\n",
    "- Excellent results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dba68c3",
   "metadata": {},
   "source": [
    "### Transformer Architecture\n",
    "\n",
    "#### Encoder-Decoder Models\n",
    "- From the **\"Attention Is All You Need\"** paper:\n",
    "\n",
    "<img src='img/5.png' width=\"500\" height=\"250\" align=\"center\"/>\n",
    "\n",
    "- The left-hand side is known as an **encoder** and the right-hand side is known as a **decoder**. \n",
    "- We feed in the English sentence, such as \"I like NLP,\" and the decoder can act as a transformer of the sentence from English to German:\n",
    "\n",
    "<img src='img/6.png' width=\"500\" height=\"250\" align=\"center\"/>\n",
    "\n",
    "- However, the transformer is not made up of a single encoder, but rather six encoders. \n",
    "- Each of these parts can be used independently depending on the task.\n",
    "- Encoder-Decoder models are good at generative tasks such as translation or summarization\n",
    "- Examples of encoder-decoder models are:\n",
    "    - BART (Facebook)\n",
    "    - T5 (Google)\n",
    "\n",
    "<img src='img/7.png' width=\"500\" height=\"250\" align=\"center\"/>\n",
    "\n",
    "<img src='img/8.png' width=\"500\" height=\"250\" align=\"center\"/>\n",
    "\n",
    "#### Encoder-only Models\n",
    "\n",
    "- Encoder-only models are good for tasks that require understanding of the input, such as:\n",
    "    - Sentence classification\n",
    "    - Named entity recognition (NER)\n",
    "- Examples include the family of BERT models:\n",
    "    - BERT\n",
    "    - RoBERTa\n",
    "    - DistilBERT\n",
    "    \n",
    "<img src='img/9.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "#### Decoder-only Models\n",
    "- Good for generative tasks such as text generation\n",
    "- Examples include: \n",
    "    - GPT\n",
    "    - GPT-2\n",
    "    - GPT-3\n",
    "\n",
    "<img src='img/10.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "    \n",
    "    \n",
    "#### $\\star$ $\\star$ $\\star$ In summary, transformers are made up of encoders and decoders, and the tasks we can perform will depend on whether we use either or both components $\\star$ $\\star$ $\\star$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd06280c",
   "metadata": {},
   "source": [
    "### Self-Attention\n",
    "- One of the key ingredients to transformers is **self-attention.**\n",
    "- In the following example (\"The monkey ate that banana because it was too hungry\"), how is the model able to determine that the \"it\" corresponds to the monkey, and not the banana?\n",
    "    - It does this by using a mechanism called **self-attention**, that incorporates the embeddings for all the other words in the sentence.\n",
    "    - So, when processing the word \"it,\" self-attention will take a weighted average of the embeddings of the other context words\n",
    "    - In the example below, the darker the shade, the more weight that word is given (and every word is given some weight):\n",
    "\n",
    "<img src='img/11.png' width=\"500\" height=\"250\" align=\"center\"/>\n",
    "\n",
    "- So, what's going on under the hood?\n",
    "    - As part of the self-attention mechanism, the authors of the original transformer take the word embeddings and project it into three vector spaces, which they call **Q (query)**, **K (key)**, and **V (value)**.\n",
    "    - Projecting word embeddings into new vector spaces is a tool that mathematicians use to get different representations of the word embeddings\n",
    "    - In order to calculate the attention weights, we'll take in as input the query, key, and value vectors.\n",
    "    - We then calculate the **score** of each word to determine how much focus to place on other words in the sentence.\n",
    "    - We want to try to figure out how the query and the key vectors relate to each other. \n",
    "        - This is done by taking the dot product of the query vector and the key vector.\n",
    "        - Queries and keys that are similar will have a large dot product\n",
    "        - Queries and keys that don't share much in common will have little to no overlap.\n",
    "        - In the equation below:\n",
    "            - T means that we're performing a **transpose** operation on the vector K\n",
    "            - **n** is the dimension of these vectors; we divide by the square root of n to scale the dot product attention, and so reduce its size \n",
    "            - we now have the logits and can convert this to probabilities by using the softmax function \n",
    "            - We then multiply each value vector by the softmax score\n",
    "            - We can then sum up the weighted value vectors, and this produces the self-attention calculation for a word.\n",
    "            - $\\star$ **This process takes place for every single word in the sentence.** $\\star$\n",
    "            - Self-attention allows us to apply a different weight to words in a sentence.\n",
    "\n",
    "<img src='img/12.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41ac6fc",
   "metadata": {},
   "source": [
    "### Multi-head Attention and Feed Forward Network\n",
    "- Above we looked at how self-attention can help us provide context for a word for the sentence \"the monkey ate that banana because it was too hungry,\" but what if we could get multiple instances of the self-attention mechanism, so that each can perform a different task?\n",
    "    - One could make a link between nouns and adjectives\n",
    "    - Another could connect pronouns to their subjects \n",
    "    - etc.\n",
    "- This is the idea behind **multi-headed attention.**\n",
    "- What's particularly impressive is we don't create these relations in the model; they're fully learned from the data.\n",
    "- BERT has 12 such heads, and each multi-head attention block gets three inputs:\n",
    "    - the query\n",
    "    - the key\n",
    "    - the value\n",
    "    \n",
    "<img src='img/13.png' width=\"800\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "- These (12 heads of BERT) are put through linear or dense layers before the multi-head attention function, as shown below.\n",
    "- The query, key, and value are passed through separate, fully-connected, linear layers for each attention head.\n",
    "- This model can jointly attend to information from different representations and at different positions\n",
    "\n",
    "<img src='img/14.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "- By having 12 self-attention heads, the BERT model is able to focus on several tasks at once, thus allowing it to make richer connections between words\n",
    "- Some of the larger language models have significantly more heads\n",
    "    - For example: GPT-3 has 96 such heads\n",
    "- The key takeaway from this section is that multi-head attention allows us to make richer connections between words, and none of these connections are created, but rather they're all learned by the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82097aa7",
   "metadata": {},
   "source": [
    "## 3. Large Language Models\n",
    "### GPT-3\n",
    "- GPT-3 is probably the most well-known large language model\n",
    "- **GPT**: \n",
    "    - **G**enerative: Predicts a future token, given past tokens\n",
    "    - **P**re-trained: Trained on a large corpus of data\n",
    "    - **T**ransformer: *Decoder* portion of the transformer architecture\n",
    "- **GPT-3's objective:** \n",
    "    - **Goal: Given the preceding token, predict the next token.**\n",
    "       - Causal language model\n",
    "       - Autoregressive\n",
    "- **Data trained on:**\n",
    "    - **English Wikipedia**\n",
    "    - **Common Crawl** $\\Rightarrow$ raw webpage data\n",
    "    - **WebText2** $\\Rightarrow$ dataset created by scraping URLs from Reddit (with a score of 3, used as proxy for quality)\n",
    "    - **Books1** $\\Rightarrow$ collection of novels by unpublished authors\n",
    "    - **Books2** $\\Rightarrow$ collection of novels by unpublished authors\n",
    "- **Task trained on:**\n",
    "    - Causal language modeling: predict the next word in a text\n",
    "    - We can train the model in a self-supervised way and we don't have to annotate our datasets\n",
    "    - We can then take all these humungous datasets and use them to train our model\n",
    "    - Additionally, we want to use some decoding algorithms, such as [**beam search**](https://en.wikipedia.org/wiki/Beam_search) to give us a balance of coherent language and diversity so we don't get sentences repeated.\n",
    "\n",
    "<img src='img/15.png' width=\"500\" height=\"250\" align=\"center\"/>\n",
    "\n",
    "- For a couple of years, researches have focused on getting a large corpus of data and training a language model\n",
    "- To use a language model for a specific task (for example: sentiment analysis), you'd have to:\n",
    "    - Give it hundreds of examples of sentences\n",
    "    - Sentences labeled as positive or negative\n",
    "    - Train the model on these sentences and labels\n",
    "    - Model will produce good results\n",
    "    \n",
    "#### What if we could create a language model that if we give it a new task and a couple of examples with the expected output, that it would be able to perform well on these tasks: GPT-3 does just this.\n",
    "\n",
    "- **Prompt:**\n",
    "    - Way to interact with models\n",
    "    - Zero-shot learning $\\Rightarrow$ given task with no examples\n",
    "    - One-shot learning $\\Rightarrow$ given task with one example and expected output\n",
    "    - Few-shot learning $\\Rightarrow$ given task with a couple of examples and expected output\n",
    "    \n",
    "<img src='img/16.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "- **In summary:** GPT-3 provides an easy way to interact with models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3672a74e",
   "metadata": {},
   "source": [
    "### GPT-3 Use Cases\n",
    "- Open AI provides access to GPT-3 via an API: [https://beta.openai.com/playground](https://beta.openai.com/playground)\n",
    "\n",
    "#### Classification\n",
    "- In the following example, we enter the key Fedex with an empty value and the transformer correctly fills in the answer (highlighted in green):\n",
    "\n",
    "<img src='img/17.png' width=\"500\" height=\"250\" align=\"center\"/>\n",
    "\n",
    "- If you look up the notes for Open AI's model training data, this training cuts off in 2021, but Meta and technology seems to be a reasonable answer (below).\n",
    "\n",
    "<img src='img/18.png' width=\"500\" height=\"250\" align=\"center\"/>\n",
    "\n",
    "#### Summarize text for a 2nd grader\n",
    "- We use the first few paragraphs of the Wikipedia entry for GPT-3:\n",
    "\n",
    "<img src='img/19.png' width=\"800\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "<img src='img/20.png' width=\"800\" height=\"400\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ff942d",
   "metadata": {},
   "source": [
    "#### Ad from product description\n",
    "\n",
    "<img src='img/21.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "<img src='img/22.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "<img src='img/23.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4281d834",
   "metadata": {},
   "source": [
    "#### Study notes\n",
    "\n",
    "<img src='img/24.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02b914c",
   "metadata": {},
   "source": [
    "## Challenges and Shortcomings of GPT-3\n",
    "#### 1. Bias\n",
    "- GPT-3 was trained on data that is biased\n",
    "- Human language and text naturally reflect bias\n",
    "- GPT-3 trained on data deemed interesting on Reddit via upvotes from other users\n",
    "\n",
    "#### 2. Environmental impact\n",
    "- Carbon emissions study of large language models was conducted by Google and Berkeley in 2021 and found that **training GPT-3 would've resulted in energy consumption of almost 1,300 MMWh (megawatt hours) and the release of 550 tons of $CO_2$**\n",
    "\n",
    "\n",
    "#### It's worth noting that some of the large language models that followed GPT-3 tried to optimize/address some of these challenges (bias and environmental impact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e347ea18",
   "metadata": {},
   "source": [
    "## GLaM\n",
    "- The Google research team noted that training large dense models requires significant amount of compute resources, and they proposed a family of language models called GLaM\n",
    "- **GLaM**: **G**eneralist **La**nguage **M**odel\n",
    "- Sparse model\n",
    "- Significantly less training costs compared to an equivalent dense model\n",
    "- **1/3 of energy used to train GPT-3 and still have better overal zero shot and one-shot performance across the board.**\n",
    "- Largest GLaM model has 1.2 trillion parameters, which is approximately 7x larger than GPT-3\n",
    "\n",
    "### GLaM Architecture\n",
    "- Source: \"GLaM: Efficient Scaling of Language Models With Mixture-of-Experts\" (Du et al.)\n",
    "- Upper block is a transformer layer (noticed multi-headed attention and feedforward network)\n",
    "- Bottom block has \"mixture-of-experts\" layer (moe uses softmax)\n",
    "\n",
    "<img src='img/25.png' width=\"500\" height=\"250\" align=\"center\"/>\n",
    "\n",
    "- Even though each mixture of expert layer has many more parameters, the experts are sparsely activated. This means that for a given input token, only a limited subset of experts is used. During training, each mixture of experts layers gating network is trained to use its input to activate the best two experts for each token of an input sequence. During inference, the learned gating network dynammically pick the two best experts for each token. As a result, even though the GLaM model has 1.2 trillion parameters, only 96.6 billion of those are activated during training (much less than the 175 billion of GPT-3)\n",
    "\n",
    "<img src='img/26.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "- **In summary: the objective of Google's GLaM model is to reduce the training and inference cost using a sparse mixture of experts model.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268c136a",
   "metadata": {},
   "source": [
    "## Megatron-Turing NLG Model\n",
    "- A lot of the research after GPT-3 was released seemed to indicate that scaling up models improved performance.\n",
    "- So Microsoft and NVidia partnered together to create the Megatron-Turing NLG model, with a massive three times more parameters than GPT-3\n",
    "\n",
    "<img src='img/27.png' width=\"800\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "### Model Parameters\n",
    "- Modelwise, the artchitecture uses the transformer's decoder just like GPT-3,  ut you can see that it has more layers and more attention heads than GPT-3.\n",
    "\n",
    "<img src='img/28.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "#### Hardware challenges\n",
    "The researchers identified a couple of challenges with working with large language models:\n",
    "- Can't fit parameters of largest language models in memory of largest GPUs\n",
    "- Need parallelism techniques on both memory and compute to use thousands of GPUs\n",
    "- Although these researchers achieved superior zero-, one-, and few-shot learning accuracies on several NLP benchmarks and established some new state-of-the-art results, **a lot of their success is probably more around the super-computing hardware infrastructure that was developed** with an enormous 600 NVidia DGX A100 nodes.\n",
    "\n",
    "<img src='img/29.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "#### In summary: the objective around the Megatron-Turing language model seems to be mostly around hardware infrastructure, and this model was one of the largest dense decoder models, coming in at 530 billion parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79c1440",
   "metadata": {},
   "source": [
    "## Gopher\n",
    "- The DeepMind research team released Gopher in January 2022\n",
    "- They released six flavors of the model ranging from 44M to 280B parameters\n",
    "- Put together a diverse dataset called MassiveText dataset\n",
    "- Tested on 152 tasks\n",
    "- Architecture is similar to GPT-3 in that it just uses the decoder portion of the transformer\n",
    "\n",
    "<img src='img/31.png' width=\"800\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "- The MassiveText corpus has 2.3 trillion tokens, but the model only trains on a subset of these tokens, so the model doesn't get to see the whole dataset\n",
    "- Over 99% of MassiveText is in English. The remaining text is split between Hindi, followed by a couple of mostly European languages.\n",
    "- If we look at the top six domains of MassiveWeb, we can see that at least four are either academic or scientific in nature. \n",
    "\n",
    "<img src='img/32.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "- So it shouldn't be much of a surprise that many of the tasks that Gopher is tested on are scientific in nature, such as High School Chemistry, etc.\n",
    "\n",
    "<img src='img/33.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "- There are 152 different tasks that the model is evaluated on, and they range from reading comprehension and fact checking to mathematics, common sense, and logical reasoning\n",
    "- **Gopher** outperforms state-of-the-art large language models in 100 of the 124 tasks it was tested on.\n",
    "- Below, the x-axis refers to different tasks within the category \n",
    "\n",
    "<img src='img/34.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "- In general, Gopher doesn't do as well on tasks such as language modeling, common sense, and logical reasoning.\n",
    "\n",
    "<img src='img/35.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "<img src='img/36.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "<img src='img/37.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "<img src='img/38.png' width=\"600\" height=\"300\" align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f708740",
   "metadata": {},
   "source": [
    "## Scaling laws\n",
    "- Why do we have such large parameter models?\n",
    "- Around the time of the release of GPT-3, the OpenAI team released some results around what they called the scaling laws for large models.\n",
    "- **The performance of large models is a function of:**\n",
    "    - **Model parameters**\n",
    "    - **Size of the dataset**\n",
    "    - **Total amount of compute available for training**\n",
    "    \n",
    "<img src='img/39.png' width=\"800\" height=\"400\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298727ea",
   "metadata": {},
   "source": [
    "- The OpenAI team then go on to propose that as more compute becomes available, you can decide where you want to allocate this; either training a larger model using larger batches or training for more steps\n",
    "- **The conclusion they came to was that most of the increase should towards increasing the model size**\n",
    "- **There will be some benefit to using more data and using large batch sizes but minimal contribution if you train for more steps***\n",
    "- **One reason that model sizes have just gotten bigger since GPT-3 is that these scaling laws suggest that increasing the model size will give you the biggest benefit**\n",
    "\n",
    "<img src='img/41.png' width=\"800\" height=\"400\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde3f648",
   "metadata": {},
   "source": [
    "## Chinchilla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e18e7d",
   "metadata": {},
   "source": [
    "- Up until this point, we've seen that the trend has been to increase model size.\n",
    "- Interestingly, the number of training tokens used for most of these models has been around 300 billion\n",
    "- The DeepMind team's hypothesis was too large, and that if you take the same compute budget, a smaller model trained on more data will perform better\n",
    "- They ten tested this hypothesis by training over 400 language models ranging from 70 million to over 16 billion parameters with datasets from five to 500 billion tokens\n",
    "- They then trained **Chinchilla**, a 70 billion parameter model with 1.4 trillion training tokens\n",
    "- And **Chinchilla outperforms all previous models, including Gopher, on a large range of dowanstream evaluation tasks.**\n",
    "- As this is a smaller model, this means less compute required for fine-tuning and inference.\n",
    "- The conclusion that the DeepMind team came to very different from the conclusion that OpenAI came to:\n",
    "\n",
    "### Recommendation from Chinchilla Paper\n",
    "- Tenfold increase in computational budget\n",
    "    - model size $\\Rightarrow$ scaled in equal proportions\n",
    "    - number of training tokens $\\Rightarrow$ scaled in equal proportions\n",
    "\n",
    "<img src='img/42.png' width=\"800\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "- FLOPs = floating point operations; a measure of computation\n",
    "- The DeepMind team set out to answer this question:\n",
    "    - **Given a fixed FLOPs budget, how should one trade-off model size and the number of training tokens?**\n",
    "    \n",
    "<img src='img/43.png' width=\"800\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "- **For a given flop budget, what is the optimal parameter count?**\n",
    "\n",
    "<img src='img/44.png' width=\"800\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "<img src='img/45.png' width=\"800\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "- They concluded that you can end up with a more performant model using a small model with more training data.\n",
    "\n",
    "<img src='img/46.png' width=\"800\" height=\"400\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47390512",
   "metadata": {},
   "source": [
    "## BIG-bench\n",
    "\n",
    "#### Challenges with current benchmarks\n",
    "- Too narrow in scope\n",
    "    - language understanding\n",
    "    - summarization\n",
    "- **BIG-bench**: **B**eyond the **I**mitation **G**ame **Bench**mark\n",
    "    - [See website here](https://wwwgithub.com/google/BIG-bench)\n",
    "    - 200 tasks that humans perform well on but current state-of-the-art models don't\n",
    "    - Team of human expert raters \n",
    "        - Performed all tasks to provide strong baseline\n",
    "        - Used all available resources (including searching the internet)\n",
    "    - Task examples:\n",
    "        - Checkmate in one move\n",
    "        - Guess movies from their emoji descriptions $/Rightarrow$ The focus of this task is on describing the movie plot with emojis, rather than the movie title, so the computer needs to understand the movie plot.\n",
    "        - Kannada riddles\n",
    "    - **Results:**\n",
    "        - **No model, regardless of size, outperformed the best-performing human on any task.**\n",
    "        - **However, on some tasks, the bes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6314d03",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1229c0ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdc5687a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ffae2eb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "555b2ef3",
   "metadata": {},
   "source": [
    "<img src='img/x.png' width=\"800\" height=\"400\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d9a473",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
