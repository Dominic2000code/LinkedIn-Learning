{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d469f07",
   "metadata": {},
   "source": [
    "# Advanced AI: Transformers for NLP Using Large Language Models\n",
    "\n",
    "**Instructor:** Jonathan Fernandes\n",
    "\n",
    "Transformers have quickly become the go-to architecture for natural language processing (NLP). As a result, knowing how to use them is now a business-critical skill in your AI toolbox. In this course, instructor Jonathan Fernandes walks you through many of the key large language models developed since GPT-3. He presents a high-level overview of GLaM, Megatron-Turing NLG, Gopher, Chinchilla, PaLM, OPT, and BLOOM, relaying some of the most important insights from each model.\n",
    "\n",
    "Get a high-level overview of large language models, where and how they are used in production, and why they are so important to NLP. Additionally, discover the basics of transfer learning and transformer training to optimize your AI models as you go. By the end of this course, you’ll be up to speed with what’s happened since OpenAI first released GPT-3 as well as the key contributions of each of these large language models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19f6184",
   "metadata": {},
   "source": [
    "## 1. Transformers in NLP\n",
    "### What are large language models?\n",
    "- BERT and GPT-3 are examples of large language models\n",
    "- **Large language model architecture is based on transformer architecture.**\n",
    "- Transformers and large language models were proposed by a team of Google researchers in 2017 in a paper entitled: \"**Attention Is All You Need**,\" which has become a turning point in NLP.\n",
    "- Large language models have millions and often billions of parameters and are trained on enormous datasets\n",
    "- GPT-3 was released in May of 2020\n",
    "\n",
    "<img src='img/1.png' width=\"800\" height=\"400\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa08f5a3",
   "metadata": {},
   "source": [
    "- Models released by Google research include: **GLaM**, **PaLM**\n",
    "- Models released by DeepMind include: **Gopher**, **Chinchilla**\n",
    "- Released by Microsoft and Nvidia: Megatron-Turing NLG or **MT-NLG**\n",
    "- Released by Meta AI: **OPT** $\\Rightarrow$ makes large language models available to researchers outside of big tech\n",
    "- Released by Hugging Face: **BLOOM** $\\Rightarrow$ makes large language models available to researchers outside of big tech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840ce9d5",
   "metadata": {},
   "source": [
    "### Transformers in Production\n",
    "\n",
    "#### BERT\n",
    "- In 2019, Google started using BERT as part of search \n",
    "    - $\\Rightarrow$ Now, when entering something into Google search, you can enter something more \"English sounding\"\n",
    "    - For example: instead of \"curling objective\" $\\Rightarrow$ \"what's the main objective of curling?\"\n",
    "    - Another example:\n",
    "        - In the past, if you did a Google search using the phrase \"Can you get medicine for someone pharmacy,\" it would **not** have picked up on the fact that \"for someone\" was a really important part of a query $\\Rightarrow$ But now, it will pick up on the fact that you're looking for another person to pick up the medicine\n",
    "- **BERT**: **B**idirectional **E**ncoder **R**epresentations from **T**ransformers \n",
    "- BERT was fed the Wikipedia and the BookCorpus data as input\n",
    "- One of the first large language models developed by the Google research team\n",
    "- The quality of Google search has improved significantly using BERT.\n",
    "\n",
    "### Transformers: History\n",
    "- The models based on the original transformer paper from 2017 have evolved over the years.\n",
    "- One of the challenges with training large language models in 2017 was that you needed labeled data.\n",
    "- The ULMFiT model proposed by Jeremy Howard and Sebastian Ruda provided a framework where you didn't need labeled data, and that meant **large corpus of texts, such as Wikipedia, could now be used to train models.**\n",
    "- In June of 2018, GPT or **G**enerative **P**re-**T**rained Model, developed by Open AI, was the first pre-trained transformer model.\n",
    "- When Open AI released a bigger and better version of GPT (GPT-2) in Feb 2019, it made headlines because the team didn't want to release the details of the model due to **ethical concerns.**\n",
    "- Meta's BART and Google's T5 are both large pre-trained models using the same architecture as the original transformer\n",
    "- Hugging Face released DistilBERT, which is a smaller, faster, and lighter version of BERT: DistilBERT had 95% the performace of BERT and reduced the size of the BERT model by 40%\n",
    "- In May 2020 Open AI released GPT-3, which is excellent at generating high-quality English sentences.\n",
    "    - Although Open AI provided a lot of details in their GPT-3 paper, they didn't reveal the dataset they used or thier model weights\n",
    "    \n",
    "<img src='img/2.png' width=\"800\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "<img src='img/3.png' width=\"800\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "**Note** that in the graph above, the y-axis is on a log scale, and so the growth is not linear but exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3ae468",
   "metadata": {},
   "source": [
    "## 2. Training Transformers and Their Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff99e6c",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "- Transfer learning is made up of 2 components:\n",
    "    - **Pre-training** $\\Rightarrow$ Extremely resource-heavy\n",
    "    - **Fine-tuning** $\\Rightarrow$ Involves training our model with labeled data\n",
    "    \n",
    "<img src='img/4.png' width=\"800\" height=\"400\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d333ae32",
   "metadata": {},
   "source": [
    "#### Pre-training Tasks: BERT (Google)\n",
    "- **Masked language modeling:** Fed Wikipedia and BookCorpus data as input and words were randomly masked out\n",
    "- BERT then had to predict what the most likely candidates were for these masked words\n",
    "- With **Next sentence prediction**, it had to predict whether one sentence followed the other.\n",
    "    - 50% of the time one sentence did follow the other and these were labeled as `isNext`\n",
    "    - 50% of the time a random other sentence from the corpus was used, and these were labeled as `notNext`\n",
    "- According to BERT's documentation, **1,500 words is approximately equivalent to 2,400 tokens.**\n",
    "    - So this means **one word is approximately 1.4 tokens.**\n",
    "    - **A novel of 100,000 words is approximately 140,000 tokens.**\n",
    "    \n",
    "#### RoBERTa (Facebook)\n",
    "- Trained in one day\n",
    "- 2 trillion tokens\n",
    "- Also used were Wikipedia, BookCorpus, as well as the Common Crawl news dataset, OpenWebText, and the Common Crawl stories:\n",
    "    - **Common Crawl** is a raw webpage dataset from years of web crawling \n",
    "    - **OpenWebText** is a dataset created by scraping URLs from REddit with a score of three (this is a proxy for the quality of the data response)\n",
    "    \n",
    "#### GPT-3 (Open AI)\n",
    "- 34 days training days\n",
    "- Used 10,000 V100 GPUs\n",
    "- 300B training tokens\n",
    "- Primarily an Azure infrastructure\n",
    "- Used Wikipedia, CommonCrawl, WebText2, Books1, Books2\n",
    "\n",
    "#### Benefits of Transfer Learning\n",
    "- Faster development \n",
    "    - For BERT, the author suggest two to four epochs of training\n",
    "    - Much better than the thousands of hours of pre-training time\n",
    "- Less data to fine-tune\n",
    "- Excellent results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce58b1c0",
   "metadata": {},
   "source": [
    "### Transformer Architecture\n",
    "\n",
    "#### Encoder-Decoder Models\n",
    "- From the **\"Attention Is All You Need\"** paper:\n",
    "\n",
    "<img src='img/5.png' width=\"500\" height=\"250\" align=\"center\"/>\n",
    "\n",
    "- The left-hand side is known as an **encoder** and the right-hand side is known as a **decoder**. \n",
    "- We feed in the English sentence, such as \"I like NLP,\" and the decoder can act as a transformer of the sentence from English to German:\n",
    "\n",
    "<img src='img/6.png' width=\"500\" height=\"250\" align=\"center\"/>\n",
    "\n",
    "- However, the transformer is not made up of a single encoder, but rather six encoders. \n",
    "- Each of these parts can be used independently depending on the task.\n",
    "- Encoder-Decoder models are good at generative tasks such as translation or summarization\n",
    "- Examples of encoder-decoder models are:\n",
    "    - BART (Facebook)\n",
    "    - T5 (Google)\n",
    "\n",
    "<img src='img/7.png' width=\"500\" height=\"250\" align=\"center\"/>\n",
    "\n",
    "<img src='img/8.png' width=\"500\" height=\"250\" align=\"center\"/>\n",
    "\n",
    "#### Encoder-only Models\n",
    "\n",
    "- Encoder-only models are good for tasks that require understanding of the input, such as:\n",
    "    - Sentence classification\n",
    "    - Named entity recognition (NER)\n",
    "- Examples include the family of BERT models:\n",
    "    - BERT\n",
    "    - RoBERTa\n",
    "    - DistilBERT\n",
    "    \n",
    "<img src='img/9.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "#### Decoder-only Models\n",
    "- Good for generative tasks such as text generation\n",
    "- Examples include: \n",
    "    - GPT\n",
    "    - GPT-2\n",
    "    - GPT-3\n",
    "\n",
    "<img src='img/10.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "    \n",
    "    \n",
    "**In summary, transformers are made up of encoders and decoders, and the tasks we can perform will depend on whether we use either or both components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f98aff7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1558f3b1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9d82b0a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3672a74e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc556102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "555b2ef3",
   "metadata": {},
   "source": [
    "<img src='img/x.png' width=\"800\" height=\"400\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9523eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
