{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ec6ea80",
   "metadata": {},
   "source": [
    "# Deep Learning: Image Recognition\n",
    "**Instructor:** Adam Geitgey\n",
    "\n",
    "Thanks to deep learning, image recognition systems have improved and are now used for everything from searching photo libraries to generating text-based descriptions of photographs. In this course, learn how to build a deep neural network that can recognize objects in photographs. Find out how to adjust state-of-the-art deep neural networks to recognize new objects, without the need to retrain the network. Explore cloud-based image recognition APIs that you can use as an alternative to building your own systems. Learn the steps involved to start building and deploying your own image recognition system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baff2e4e",
   "metadata": {},
   "source": [
    "#### Build cutting-edge image recognition systems\n",
    "* **Image recognition** is the ability for computers to look at a photograph and understand what's in the photograph\n",
    "* In the last few years, researchers have made major break throughs in image recognition thanks to neural networks\n",
    "* **Keras** is a high-level library for building neural networks in Python with only a few lines of code; built on top of either TensorFlow or Theano\n",
    "* One of the most important things to configure in a neural network is activation functions\n",
    "    * Before values flow from one layer to the next they pass through an activation function\n",
    "    * **Activation functions** decide which inputs from the previous layer are important enough to feed to the next layer\n",
    "* The final step of defining a neural network is to compile it by calling `model.compile()`; this tells Keras that we're done building the model and that we actually want to carry it out in memory\n",
    "* The optimizer algorithm is used to train the neural network\n",
    "* The loss function is how the training process measures how right or how wrong your NN's predictions are\n",
    "\n",
    "#### Using Images as Input to a NN\n",
    "* Bright points are closer to 255 and dark points are closer to 0\n",
    "* We can think of an image as a 3D array that is always three layers deep; so to be able to feed this image into a NN, we need the NN to have 1 input node for every number in this 3-D array (ie pixel)\n",
    "* These numbers add up very quickly\n",
    "* For a small **256 x 256 pixel image**, (by modern terms, a pretty tiny image):\n",
    "    * We need 256 x 256 x 3 = **196,608 input nodes**\n",
    "    * And that's just for the input layer\n",
    "    * The number of nodes in the entire neural network will quickly grow into the millions\n",
    "    * That's why using NNs for image processing in so computationally intensive\n",
    "        * **Because of this, image recognition systems tend to use small image sizes**\n",
    "        * It's very common to build image recognition systems that work with images that are **between 128 and 512 pixels wide.**\n",
    "        * Any larger than that, and it gets too slow and requires too much memory\n",
    "        * When working with larger images, we usually just scale them down to those smaller sizes before feeding them into the neural network "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1c1182",
   "metadata": {},
   "source": [
    "### Recognizing Image Contents with a Neural Network\n",
    "* During the **inference phase** the neural network will give us a **prediction**; this prediction will be in the form of a probability\n",
    "* We can also build a single neural neetwork that has more than one output\n",
    "\n",
    "<img src='data/nn1.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "* You can roughly think of the the top (leftmost) layers as looking for simple patterns like lines and sharp edges and the lower layers use the signals from the higher layers to look for more and more complex shapes and patterns\n",
    "* With all the layers working together, the model can identify very complex objects\n",
    "* That means that adding more layers to a NN tends to give it the capacity to learn more complex patterns and shapes; this is where the term **deep learning** originally came from\n",
    "* **Deep learning** is just the idea that making models deeper by adding more capacity to them lets us recognize more complex patterns in data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3adb19",
   "metadata": {},
   "source": [
    "### Adding convolution for translational invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae983db",
   "metadata": {},
   "source": [
    "* If we only train the NN with pictures of numbers that are perfectly centered, the NN will get confused if it sees anything else (for example, an uncentered \"8\")\n",
    "* For example:\n",
    "\n",
    "<img src='data/nn2.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "<img src='data/nn3.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae3388b",
   "metadata": {},
   "source": [
    "* The neural network won't be able to make a good prediction on the uncentered 8 from the lower example above (where the model is only trained on centered 8s). \n",
    "* But, the 8 could appear anywhere in the image; it could just as easily appear at the bottom, like this:\n",
    "\n",
    "<img src='data/nn4.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38389303",
   "metadata": {},
   "source": [
    "* **We need to improve our neural network so that it can recognize objects in any position in the image.**\n",
    "    * This is called **Translation invariance.**\n",
    "    \n",
    "#### Translation Invariance and Convolutional Layers\n",
    "* **Translation invariance** is the idea that a machine learning model can recognize an object no matter whether it is moved (or *translated*) in the image.\n",
    "* The solution is to add a new type of layer to our neural network: a **convolutional layer**\n",
    "* Unlike a normal Dense layer, where every node is connected to every other node, this (convolutional) layer breaks apart the image in a special way so that it can recognize the same object in different positions\n",
    "* We do this by passing a small window (shown in orange below) over the image. \n",
    "* Each time it lands somewhere, we grab a new image tile; we repeat this until we've covered the entire image\n",
    "\n",
    "<img src='data/nn5.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "* Next, we pass each image tile through the same NN layer(s). Each tile will be processed the same way and will save a value each time\n",
    "* In other words, we're turning the image into an array, where each entry in the array represents whether or not the neural network thinks a certain pattern appears at that part of the image\n",
    "* Next, we'll repeat the exact process again, but this time we'll use a different set of weights on the nodes in our NN layer\n",
    "* This will create another feature map that tells us whether or not a certain pattern appears in the image\n",
    "* But because we're using different weights, it will be looking for a different pattern than the first time\n",
    "* We can repeat this process several times until we have several layers in our new array \n",
    "\n",
    "<img src='data/nn6.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "<img src='data/nn7.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "* This turns our original array into a 3D array\n",
    "* Each element in the array represents where (whether?) a certain pattern occurs\n",
    "* But because we are checking each tile of the original image, it doesn't matter where in the image a pattern occurs, we can find it anywhere\n",
    "* This **3D array is waht we'll feed into the next layer of the neural network.**\n",
    "* It will use this information to determine which patterns are most important in determining the final output\n",
    "* Adding a convolutional layer makes it possible for our neural network to be able to find the pattern, no matter where it appears in an image\n",
    "* **Normally, we'll have several convolutional layers** that repeat the above process multiple times. \n",
    "* **The rough idea is that we keep squishing down the image with each convolutional layer while still capturing the most important information from it.** By the time we reach the output layer, the neural network will have been able to identify whether or not the object appeared\n",
    "* Convolutional neural networks are the standard approach to building image recognition systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639a8602",
   "metadata": {},
   "source": [
    "## 3. Designing a Deep Neural Network for Image Recognition\n",
    "\n",
    "### Designing a neural network architecture for image recognition\n",
    "* Before we start coding our image recognition NN, let's sketch out how a basic neural network works\n",
    "* **A basic neural network comprised of all dense, or fully-connected, layers doesn't work efficiently for images because objects can appear in lots of different places in an image.**\n",
    "* The solution is to add one or more convolution layers, which help us detect patterns no matter where they appear in our image\n",
    "* **It can be very effective to place two or more convolutional layers in a row** so in our example we'll add them in pairs\n",
    "* The convolutional layers are looking for patterns in our image and recording whether or not they found those patterns in each part of our image; but we don't usually need to know *where* in an image a pattern was found down to the specific pixel; it's good enough to know the rough location where it was found. To solve this problem we can use a technique called **max pooling**\n",
    "\n",
    "#### Max Pooling\n",
    "\n",
    "<img src='data/nn8.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "* We could pass the above information (regarding whether or not a pixel corresponds to a cloud) directly to the rest of our neural network, but it we can reduce the amount of information that we pass to the next layer, it will make the neural network's job much easier (and faster)\n",
    "* The idea of **max pooling** is to down sample the data by only passing on the most important bits\n",
    "\n",
    "<img src='data/nn9.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a997cd6e",
   "metadata": {},
   "source": [
    "* The idea above is that, by capturing the most important data (most extreme values), we'll get nearly the same result, but much more efficiently.\n",
    "\n",
    "#### Dropout\n",
    "* **Dropout** is a technique to make the NN more robust and prevent overfitting\n",
    "* The idea is that we add a droppout layer between other layers that will randomly throw away some of the data passing throught by cutting some of the connections in the neural network\n",
    "\n",
    "<img src='data/nn10.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7089210e",
   "metadata": {},
   "source": [
    "* By randomly cutting connections with each training image, the neural network is forced to try harder to learn  multiple ways to represent the same ideas (rather than *memorize* an image).\n",
    "\n",
    "<img src='data/nn11.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "* If we want to make our network more powerful and able to recognize more complex images, we can add more layers to it\n",
    "* But, instead of just adding layers randomly, we'll add more copies of our convolutional block.\n",
    "* When all these layers are working together, we'll be able to detect complex objects \n",
    "\n",
    "<img src='data/nn12.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "* This is a very typical design for an image recognition neural network, but it's also one of the most basic\n",
    "* The latest designs involve branching pathways, shortcuts between groups of layers, and all sorts of other tricks, but they all build on these same basic ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c448409b",
   "metadata": {},
   "source": [
    "### Exploring the CIFAR-10 Data Set\n",
    "* [See here](https://www.cs.toronto.edu/~kriz/cifar.html) for more detail on the CIFAR-10 dataset (and AlexNet)\n",
    "\n",
    "#### Exploring your dataset\n",
    "   * Always look through the data by hand\n",
    "   * Check for obvious errors\n",
    "   * Verify that the data makes sense\n",
    "   \n",
    "### Loading an image dataset\n",
    "* The function `cifar10.load_data()` returns **four different arrays**\n",
    "    * `X_train`\n",
    "    * `y_train`\n",
    "    * `X_test`\n",
    "    * `y_test`\n",
    "* **`(x_train, y_train), (x_test, y_test) = cifar10.load_data()`**\n",
    "* **NNs work between when the data are floats between zero and one**:\n",
    "\n",
    "```\n",
    "# Normalize data set to 0-to-1 range\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "```\n",
    "\n",
    "* cifar10 provides the labels for each class as values from 0 to 9, **but since we are creating a NN with 10 outputs, we need a separate expected value for each of those outputs. So we need to convert each label from a single number into an array with 10 elements.** In that array, one element should be set to one and the rest set to zero. \n",
    "* **This is something you'll almost always need to do with your trainind data, so keras provides a helper function: `keras.utils.to_categorical()`**\n",
    "    * To use this function, you just pass in your array with the labels (which in our case is `y_train`) along with the numbe of classes it has (which in our case is `10`)\n",
    "    * `y_train = keras.utils.to_categorical(y_train, 10)`\n",
    "    * `y_test = keras.utils.to_categorical(y_test, 10)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4669db7a",
   "metadata": {},
   "source": [
    "#### Dense Layers\n",
    "* `relu` is the standard choice for activation function when working with images because it works well and is computationally efficient\n",
    "* We'll need one node in the output layer for *each* object we want to detect\n",
    "* **When doing classification with more than one kind of object, the output layer will almost always use a `softmax` activation function.**\n",
    "    * The **softmax** activation function is a special function that $\\star$ **makes sure all the output values from this layer add up to exactly one** $\\star$\n",
    "* When we're bilding a neural network and adding layers to it, it's helpful to print out a list of the layers in the neural networks so far; we can do this by calling:\n",
    "    * `model.summary()`\n",
    "    \n",
    "#### Convolution layers\n",
    "* **To be able to recognize images efficiently, we'll add convolutional layers before our densely connected layers**\n",
    "* **Note that there are 2 types of convolutional layers: 1D and 2D**\n",
    "    * Since we're working with images, we'll want to add the Conv2D layer\n",
    "    * For some data like sound waves, you can use Conv1D (but typically you'll use Conv2D)\n",
    "* Parameters:\n",
    "    * The first parameter is how many different filters should be in the layer\n",
    "        * Each filter will be able of detecting one pattern in the image (we'll start with 32, a power of 2)\n",
    "    * Next, we need to pass in the size of the window that we'll use when creating image tiles from each image\n",
    "        * By passing in the tuple `(3,3)`, we are selecting a 3 pixel x 3 pixel window\n",
    "        * This will split up the original image into 3 x 3 tiles; when we do that, we have to decide what to do with the edges of the image. If the image size isn't exactly divisible by 3, we'll have a few extra pixels left over on the edge. We can either throw that info away or we can add padding to the image. **Padding is just extra zeros added to the edge(s) of the image to make the math work out, and also to avoid losing info from the edges.**\n",
    "    * To add extra padding that causes the image to retain its original size: **`padding= same`**\n",
    "    * Just like a normal Dense layer, convolutional layers also need an activation function and we almost always use the `relu` activation function because of its efficiency\n",
    "* `model.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", input_shape=(32, 32, 3)))`\n",
    "* **Note:** Whenever we transition between convolutional layers and dense layers, we need to tell Keras that we're no longer working with 2D data\n",
    "    * **To do that, we need to create a `Flatten()` layer**\n",
    "    \n",
    "\n",
    "```\n",
    "# Create a model and add layers\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", input_shape=(32, 32, 3)))\n",
    "model.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "* Note that each layer also has a **total number or parameters** listed. This is the total number of weights in that layer (including bias)\n",
    "* **The total params = the size or complexity of our NN**\n",
    "* **Note too that 512 nodes in the first Dense layer, because images are input as 32 x 32 pixels, then flattened in the `Flatten()` layer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c072900",
   "metadata": {},
   "source": [
    "### Max pooling\n",
    "* **Typically we'll do max pooling right after a block of convolutional layers**\n",
    "* The only parameter that we have to pass in to a maxpooling layer is the size of the area we want to pool together (**`pool_size`**)\n",
    "\n",
    "```\n",
    "# Create a model and add layers\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3), activation=\"relu\"))\n",
    "model.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation=\"relu\"))\n",
    "model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd74e160",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce9d6c36",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b122095",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d7f3c55",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8156eb3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b86e4c8f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4775ad5c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14a26894",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3e55b14",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcfcb18a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ad71672",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0854adb3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5851c747",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9016108",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42b0bbe2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19c9ca6a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e181b66",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56731f3c",
   "metadata": {},
   "source": [
    "<img src='data/nn5.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
