{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ec6ea80",
   "metadata": {},
   "source": [
    "# Deep Learning: Image Recognition\n",
    "**Instructor:** Adam Geitgey\n",
    "\n",
    "Thanks to deep learning, image recognition systems have improved and are now used for everything from searching photo libraries to generating text-based descriptions of photographs. In this course, learn how to build a deep neural network that can recognize objects in photographs. Find out how to adjust state-of-the-art deep neural networks to recognize new objects, without the need to retrain the network. Explore cloud-based image recognition APIs that you can use as an alternative to building your own systems. Learn the steps involved to start building and deploying your own image recognition system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baff2e4e",
   "metadata": {},
   "source": [
    "#### Build cutting-edge image recognition systems\n",
    "* **Image recognition** is the ability for computers to look at a photograph and understand what's in the photograph\n",
    "* In the last few years, researchers have made major break throughs in image recognition thanks to neural networks\n",
    "* **Keras** is a high-level library for building neural networks in Python with only a few lines of code; built on top of either TensorFlow or Theano\n",
    "* One of the most important things to configure in a neural network is activation functions\n",
    "    * Before values flow from one layer to the next they pass through an activation function\n",
    "    * **Activation functions** decide which inputs from the previous layer are important enough to feed to the next layer\n",
    "* The final step of defining a neural network is to compile it by calling `model.compile()`; this tells Keras that we're done building the model and that we actually want to carry it out in memory\n",
    "* The optimizer algorithm is used to train the neural network\n",
    "* The loss function is how the training process measures how right or how wrong your NN's predictions are\n",
    "\n",
    "#### Using Images as Input to a NN\n",
    "* Bright points are closer to 255 and dark points are closer to 0\n",
    "* We can think of an image as a 3D array that is always three layers deep; so to be able to feed this image into a NN, we need the NN to have 1 input node for every number in this 3-D array (ie pixel)\n",
    "* These numbers add up very quickly\n",
    "* For a small **256 x 256 pixel image**, (by modern terms, a pretty tiny image):\n",
    "    * We need 256 x 256 x 3 = **196,608 input nodes**\n",
    "    * And that's just for the input layer\n",
    "    * The number of nodes in the entire neural network will quickly grow into the millions\n",
    "    * That's why using NNs for image processing in so computationally intensive\n",
    "        * **Because of this, image recognition systems tend to use small image sizes**\n",
    "        * It's very common to build image recognition systems that work with images that are **between 128 and 512 pixels wide.**\n",
    "        * Any larger than that, and it gets too slow and requires too much memory\n",
    "        * When working with larger images, we usually just scale them down to those smaller sizes before feeding them into the neural network "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36bbd3f",
   "metadata": {},
   "source": [
    "### Recognizing Image Contents with a Neural Network\n",
    "* During the **inference phase** the neural network will give us a **prediction**; this prediction will be in the form of a probability\n",
    "* We can also build a single neural neetwork that has more than one output\n",
    "\n",
    "<img src='data/nn1.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "* You can roughly think of the the top (leftmost) layers as looking for simple patterns like lines and sharp edges and the lower layers use the signals from the higher layers to look for more and more complex shapes and patterns\n",
    "* With all the layers working together, the model can identify very complex objects\n",
    "* That means that adding more layers to a NN tends to give it the capacity to learn more complex patterns and shapes; this is where the term **deep learning** originally came from\n",
    "* **Deep learning** is just the idea that making models deeper by adding more capacity to them lets us recognize more complex patterns in data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e665969",
   "metadata": {},
   "source": [
    "### Adding convolution for translational invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c0263e",
   "metadata": {},
   "source": [
    "* If we only train the NN with pictures of numbers that are perfectly centered, the NN will get confused if it sees anything else (for example, an uncentered \"8\")\n",
    "* For example:\n",
    "\n",
    "<img src='data/nn2.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "<img src='data/nn3.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ee21b5",
   "metadata": {},
   "source": [
    "* The neural network won't be able to make a good prediction on the uncentered 8 from the lower example above (where the model is only trained on centered 8s). \n",
    "* But, the 8 could appear anywhere in the image; it could just as easily appear at the bottom, like this:\n",
    "\n",
    "<img src='data/nn4.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cf9b2a",
   "metadata": {},
   "source": [
    "* **We need to improve our neural network so that it can recognize objects in any position in the image.**\n",
    "    * This is called **Translation invariance.**\n",
    "    \n",
    "#### Translation Invariance and Convolutional Layers\n",
    "* **Translation invariance** is the idea that a machine learning model can recognize an object no matter whether it is moved (or *translated*) in the image.\n",
    "* The solution is to add a new type of layer to our neural network: a **convolutional layer**\n",
    "* Unlike a normal Dense layer, where every node is connected to every other node, this (convolutional) layer breaks apart the image in a special way so that it can recognize the same object in different positions\n",
    "* We do this by passing a small window (shown in orange below) over the image. \n",
    "* Each time it lands somewhere, we grab a new image tile; we repeat this until we've covered the entire image\n",
    "\n",
    "<img src='data/nn5.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "* Next, we pass each image tile through the same NN layer(s). Each tile will be processed the same way and will save a value each time\n",
    "* In other words, we're turning the image into an array, where each entry in the array represents whether or not the neural network thinks a certain pattern appears at that part of the image\n",
    "* Next, we'll repeat the exact process again, but this time we'll use a different set of weights on the nodes in our NN layer\n",
    "* This will create another feature map that tells us whether or not a certain pattern appears in the image\n",
    "* But because we're using different weights, it will be looking for a different pattern than the first time\n",
    "* We can repeat this process several times until we have several layers in our new array \n",
    "\n",
    "<img src='data/nn6.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "<img src='data/nn7.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "* This turns our original array into a 3D array\n",
    "* Each element in the array represents where (whether?) a certain pattern occurs\n",
    "* But because we are checking each tile of the original image, it doesn't matter where in the image a pattern occurs, we can find it anywhere\n",
    "* This **3D array is waht we'll feed into the next layer of the neural network.**\n",
    "* It will use this information to determine which patterns are most important in determining the final output\n",
    "* Adding a convolutional layer makes it possible for our neural network to be able to find the pattern, no matter where it appears in an image\n",
    "* **Normally, we'll have several convolutional layers** that repeat the above process multiple times. \n",
    "* **The rough idea is that we keep squishing down the image with each convolutional layer while still capturing the most important information from it.** By the time we reach the output layer, the neural network will have been able to identify whether or not the object appeared\n",
    "* Convolutional neural networks are the standard approach to building image recognition systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb1cf28",
   "metadata": {},
   "source": [
    "## 3. Designing a Deep Neural Network for Image Recognition\n",
    "\n",
    "### Designing a neural network architecture for image recognition\n",
    "* Before we start coding our image recognition NN, let's sketch out how a basic neural network works\n",
    "* **A basic neural network comprised of all dense, or fully-connected, layers doesn't work efficiently for images because objects can appear in lots of different places in an image.**\n",
    "* The solution is to add one or more convolution layers, which help us detect patterns no matter where they appear in our image\n",
    "* **It can be very effective to place two or more convolutional layers in a row** so in our example we'll add them in pairs\n",
    "* The convolutional layers are looking for patterns in our image and recording whether or not they found those patterns in each part of our image; but we don't usually need to know *where* in an image a pattern was found down to the specific pixel; it's good enough to know the rough location where it was found. To solve this problem we can use a technique called **max pooling**\n",
    "\n",
    "#### Max Pooling\n",
    "\n",
    "<img src='data/nn8.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "* We could pass the above information (regarding whether or not a pixel corresponds to a cloud) directly to the rest of our neural network, but it we can reduce the amount of information that we pass to the next layer, it will make the neural network's job much easier (and faster)\n",
    "* The idea of **max pooling** is to down sample the data by only passing on the most important bits\n",
    "\n",
    "<img src='data/nn9.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7e1852",
   "metadata": {},
   "source": [
    "* The idea above is that, by capturing the most important data (most extreme values), we'll get nearly the same result, but much more efficiently.\n",
    "\n",
    "#### Dropout\n",
    "* **Dropout** is a technique to make the NN more robust and prevent overfitting\n",
    "* The idea is that we add a droppout layer between other layers that will randomly throw away some of the data passing throught by cutting some of the connections in the neural network\n",
    "\n",
    "<img src='data/nn10.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21722e27",
   "metadata": {},
   "source": [
    "* By randomly cutting connections with each training image, the neural network is forced to try harder to learn  multiple ways to represent the same ideas (rather than *memorize* an image).\n",
    "\n",
    "<img src='data/nn11.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "* If we want to make our network more powerful and able to recognize more complex images, we can add more layers to it\n",
    "* But, instead of just adding layers randomly, we'll add more copies of our convolutional block.\n",
    "* When all these layers are working together, we'll be able to detect complex objects \n",
    "\n",
    "<img src='data/nn12.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "* This is a very typical design for an image recognition neural network, but it's also one of the most basic\n",
    "* The latest designs involve branching pathways, shortcuts between groups of layers, and all sorts of other tricks, but they all build on these same basic ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4255c08b",
   "metadata": {},
   "source": [
    "### Exploring the CIFAR-10 Data Set\n",
    "* [See here](https://www.cs.toronto.edu/~kriz/cifar.html) for more detail on the CIFAR-10 dataset (and AlexNet)\n",
    "\n",
    "#### Exploring your dataset\n",
    "   * Always look through the data by hand\n",
    "   * Check for obvious errors\n",
    "   * Verify that the data makes sense\n",
    "   \n",
    "### Loading an image dataset\n",
    "* The function `cifar10.load_data()` returns **four different arrays**\n",
    "    * `X_train`\n",
    "    * `y_train`\n",
    "    * `X_test`\n",
    "    * `y_test`\n",
    "* **`(x_train, y_train), (x_test, y_test) = cifar10.load_data()`**\n",
    "* **NNs work best when the data are floats between zero and one**:\n",
    "\n",
    "```\n",
    "# Normalize data set to 0-to-1 range\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "```\n",
    "\n",
    "* cifar10 provides the labels for each class as values from 0 to 9, **but since we are creating a NN with 10 outputs, we need a separate expected value for each of those outputs. So we need to convert each label from a single number into an array with 10 elements.** In that array, one element should be set to one and the rest set to zero. \n",
    "* **This is something you'll almost always need to do with your trainind data, so keras provides a helper function: `keras.utils.to_categorical()`**\n",
    "    * To use this function, you just pass in your array with the labels (which in our case is `y_train`) along with the numbe of classes it has (which in our case is `10`)\n",
    "    * `y_train = keras.utils.to_categorical(y_train, 10)`\n",
    "    * `y_test = keras.utils.to_categorical(y_test, 10)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f8439c",
   "metadata": {},
   "source": [
    "#### Dense Layers\n",
    "* `relu` is the standard choice for activation function when working with images because it works well and is computationally efficient\n",
    "* We'll need one node in the output layer for *each* object we want to detect\n",
    "* **When doing classification with more than one kind of object, the output layer will almost always use a `softmax` activation function.**\n",
    "    * The **softmax** activation function is a special function that $\\star$ **makes sure all the output values from this layer add up to exactly one** $\\star$\n",
    "* When we're bilding a neural network and adding layers to it, it's helpful to print out a list of the layers in the neural networks so far; we can do this by calling:\n",
    "    * `model.summary()`\n",
    "    \n",
    "#### Convolution layers\n",
    "* **To be able to recognize images efficiently, we'll add convolutional layers before our densely connected layers**\n",
    "* **Note that there are 2 types of convolutional layers: 1D and 2D**\n",
    "    * Since we're working with images, we'll want to add the Conv2D layer\n",
    "    * For some data like sound waves, you can use Conv1D (but typically you'll use Conv2D)\n",
    "* Parameters:\n",
    "    * The first parameter is how many different filters should be in the layer\n",
    "        * Each filter will be able of detecting one pattern in the image (we'll start with 32, a power of 2)\n",
    "    * Next, we need to pass in the size of the window that we'll use when creating image tiles from each image\n",
    "        * By passing in the tuple `(3,3)`, we are selecting a 3 pixel x 3 pixel window\n",
    "        * This will split up the original image into 3 x 3 tiles; when we do that, we have to decide what to do with the edges of the image. If the image size isn't exactly divisible by 3, we'll have a few extra pixels left over on the edge. We can either throw that info away or we can add padding to the image. **Padding is just extra zeros added to the edge(s) of the image to make the math work out, and also to avoid losing info from the edges.**\n",
    "    * To add extra padding that causes the image to retain its original size: **`padding= same`**\n",
    "    * Just like a normal Dense layer, convolutional layers also need an activation function and we almost always use the `relu` activation function because of its efficiency\n",
    "* `model.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", input_shape=(32, 32, 3)))`\n",
    "* **Note:** Whenever we transition between convolutional layers and dense layers, we need to tell Keras that we're no longer working with 2D data\n",
    "    * **To do that, we need to create a `Flatten()` layer**\n",
    "    \n",
    "\n",
    "```\n",
    "# Create a model and add layers\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", input_shape=(32, 32, 3)))\n",
    "model.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "* Note that each layer also has a **total number or parameters** listed. This is the total number of weights in that layer (including bias)\n",
    "* **The total params = the size or complexity of our NN**\n",
    "* **Note too that 512 nodes in the first Dense layer, because images are input as 32 x 32 pixels, then flattened in the `Flatten()` layer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad83e80",
   "metadata": {},
   "source": [
    "### Max pooling\n",
    "* **Typically we'll do max pooling right after a block of convolutional layers**\n",
    "* The only parameter that we have to pass in to a maxpooling layer is the size of the area we want to pool together (**`pool_size`**)\n",
    "\n",
    "```\n",
    "# Create a model and add layers\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3), activation=\"relu\"))\n",
    "model.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation=\"relu\"))\n",
    "model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "* **Note** that the max pooling layers have zero parameters\n",
    "\n",
    "### Dropout\n",
    "* Force the NN to try harder to learn without memorizing the input data.\n",
    "* **Usually we'll add `Dropout` right after `MaxPooling` layers or after a group of `Dense` layers.**\n",
    "* The only parameter we need to pass in to a `Dropout()` layer is the percentage of NN connections to randomly cut\n",
    "* **Usually a value between 25%-50% works well.**\n",
    "* Note that we add dropout layers after each max pooling layer, but also after the first `Dense` layer; but here, **we make the NN work really hard to get the last answer correct and use a 50% dropout.**\n",
    "* Note that the Dropout, Max Pooling and Flatten layers all have 0 parameters because they aren't adding any additional info to our model \n",
    "\n",
    "```\n",
    "# Create a model and add layers\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3), activation=\"relu\"))\n",
    "model.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation=\"relu\"))\n",
    "model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a11d22",
   "metadata": {},
   "source": [
    "### A complete neural network for image recognition\n",
    "* The final step of defining a NN is to **compile** the NN.\n",
    "* When we `compile` the model, we're telling Keras we actually want to create the NN in memory. \n",
    "* We also tell Keras how we'll be training it and measuring its accuracy \n",
    "\n",
    "```\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "```\n",
    "* **Note that you can write your own custon loss function** but most of the time there are a few standard functions that you'll choose between \n",
    "* **If you're trying to classify an image into different categories, use `categorical_crossentropy`.**\n",
    "* **If you're only checking if an image belongs to one category, use `binary_crossentropy`.**\n",
    "* **`adam`**: **Ada**ptive **m**omemt estimation.\n",
    "* Finally we need to tell Keras what metrics we want it to report during the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a0b157",
   "metadata": {},
   "source": [
    "## 4. Building and Training the Deep Neural Network\n",
    "\n",
    "### Setting up a neural network for training\n",
    "* The first two parameters are our training dataset and our training labels\n",
    "* **`batch_size`**: how many images we want to feed into the network at once during training\n",
    "    * If we set the # too low, training will take a long time and might not ever finish\n",
    "    * If we set the # too high, we'll run out of memory on our computer\n",
    "    * Typical batch sizes are between **32-128 images**\n",
    "* **`epochs`**: one full pass through the entire training dataset\n",
    "    * The more passes through the data we do, the more chance the neural network has to learn, but the longer the training process will take (and you risk some serious overfitting)\n",
    "    * In general, the larger your dataset, the less training passes you'll do over it\n",
    "    * For example, for extremely large datasets with millions of images, you might only do 5 passes\n",
    "* **`validation_data`**:\n",
    "    * This is data that the model will never see during training and will only be used to test the accuracy of the trained model \n",
    "    * Pass `X_test` and `y_test` as an array \n",
    "* **`shuffle`**:\n",
    "    * Randomizes the the order of the training data when `True`\n",
    "    * It is important to set this to true (when possible) so that the model doesn't memorize the order of the data\n",
    "    * Default is `True`\n",
    "\n",
    "```\n",
    "# Train the model\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=30,\n",
    "    validation_data=(x_test, y_test),\n",
    "    shuffle=True\n",
    ")\n",
    "```\n",
    "\n",
    "* Note that here we aren't saving the results anywhere. So if we run the model right now, we'll be doing all the work and then throwing away the results.\n",
    "* We'll definitely want to learn to save our results before running the lengthly training process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e956e4",
   "metadata": {},
   "source": [
    "### Training a neural network and saving weights\n",
    "* When we train a NN, we want to make sure that we save the results so that we can reuse the trained model later\n",
    "* After he training completes, we want to save the NN to a file, so that we'll be able to use it to recognize objects in other programs.\n",
    "* **Saving a NN is two separate steps:**\n",
    "    * First, we want to save the structure of the NN itself; that includes which layers get created and the order that they're all hooked together\n",
    "    * Second, we want to save the weights of the neural network \n",
    "    * **The reason that we save the structure separately from the weights is because often you'll train the same NN multiple times on different datasets. It's convenient to be able to load different sets of weights using the same neural network structure.**\n",
    "    \n",
    "#### Saving the NN structure itself\n",
    "* Keras can convert the structure of a NN into JSON by calling the `model.to_JSON()` function. \n",
    "* Then we just need to write this JSON data to a text file; there's lots of ways to do this in Python, but one easy way using the `Path` library is shown below:\n",
    "\n",
    "\n",
    "```\n",
    "# Save neural network structure\n",
    "model_structure = model.to_json()\n",
    "f = Path(\"model_structure.json\")\n",
    "f.write_text(model_structure)\n",
    "\n",
    "# Save neural network's trained weights\n",
    "model.save_weights(\"model_weights.h5\")\n",
    "```\n",
    "* To save the model weights, we just need to call `model.save_weights()` and pass the name we want to save the model as.\n",
    "* The data that gets saved in this last step is in a binary format called HDF5\n",
    "* The HDF5 format id designed for saving and loading large binary files efficiently\n",
    "* The loss is the numerical representation of how wrong our model is right now\n",
    "\n",
    "### Making predictions with the trained neural network\n",
    "* When we pass an image through our NN its going to return a likelihood for each type of object it was trained to recognize.\n",
    "* In order to decode those numbers to names, we need a list of names that correspond with each number:\n",
    "\n",
    "```\n",
    "# These are the CIFAR10 class labels from the training data (in order from 0 to 9)\n",
    "class_labels = [\n",
    "    \"Plane\",\n",
    "    \"Car\",\n",
    "    \"Bird\",\n",
    "    \"Cat\",\n",
    "    \"Deer\",\n",
    "    \"Dog\",\n",
    "    \"Frog\",\n",
    "    \"Horse\",\n",
    "    \"Boat\",\n",
    "    \"Truck\"\n",
    "]\n",
    "```\n",
    "\n",
    "* Now, we're ready to load the NN\n",
    "* First we load the structure \n",
    "\n",
    "```\n",
    "# Load the json file that contains the model's structure\n",
    "f = Path(\"model_structure.json\")\n",
    "model_structure = f.read_text()\n",
    "\n",
    "# Recreate the Keras model object from the json data\n",
    "model = model_from_json(model_structure)\n",
    "```\n",
    "* So far we've only restored the structure of the NN\n",
    "* To restore the training as well we need to load the weights file as well that we created when we trained the NN \n",
    "\n",
    "```\n",
    "# Re-load the model's trained weights\n",
    "model.load_weights(\"model_weights.h5\")\n",
    "```\n",
    "\n",
    "#### Test one image and generate prediction start to finish:\n",
    "* Note that right now we're only testing one image with our neural network, but for efficiency reasons Keras lets you pass in batches of images at once so you can run more than one image through the neural network at one time\n",
    "* **So, we need to create a batch of images to pass in, even though we're only testing this one image**\n",
    "* Keras expects these batches as a **4-dimensional array**\n",
    "* The **first dimension is the list of images** and the other three dimensions are the image data itself\n",
    "* Since we only have this one image, **we can turn it into a 4D array by adding a new axis to it with numpy.**\n",
    "* We can do this by calling a function called **`np.expand_dims()`** and passing in the name of the array; **note that we also need to pass in `axis=0` to tell it that the new axis is the *first* dimension.** This is the convention that Keras expects\n",
    "* The results variable will contain a **list of results** for each image that we passed in.\n",
    "* Since we only passed in one image, we can just grab the first array index, hence `single_result = results[0]`\n",
    "* The `single_result` array is an array with 10 elements; each element represents how likely the image is to belong to each of the object types we listed at the top of the program.\n",
    "* Instead of returning 10 separate numbers, we can just grab the array element with the highest value (ie highest probability)\n",
    "* We do this using `np.argmax()`\n",
    "* We also grab the likelihood value of that array index so we can print it out later\n",
    "\n",
    "```\n",
    "from keras.models import model_from_json\n",
    "from pathlib import Path\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# These are the CIFAR10 class labels from the training data (in order from 0 to 9)\n",
    "class_labels = [\n",
    "    \"Plane\",\n",
    "    \"Car\",\n",
    "    \"Bird\",\n",
    "    \"Cat\",\n",
    "    \"Deer\",\n",
    "    \"Dog\",\n",
    "    \"Frog\",\n",
    "    \"Horse\",\n",
    "    \"Boat\",\n",
    "    \"Truck\"\n",
    "]\n",
    "\n",
    "# Load the json file that contains the model's structure\n",
    "f = Path(\"model_structure.json\")\n",
    "model_structure = f.read_text()\n",
    "\n",
    "# Recreate the Keras model object from the json data\n",
    "model = model_from_json(model_structure)\n",
    "\n",
    "# Re-load the model's trained weights\n",
    "model.load_weights(\"model_weights.h5\")\n",
    "\n",
    "# Load an image file to test, resizing it to 32x32 pixels (as required by this model)\n",
    "img = image.load_img(\"frog.png\", target_size=(32, 32))\n",
    "\n",
    "# Convert the image to a numpy array\n",
    "image_to_test = image.img_to_array(img)\n",
    "\n",
    "# Add a fourth dimension to the image (since Keras expects a list of images, not a single image)\n",
    "list_of_images = np.expand_dims(image_to_test, axis=0)\n",
    "\n",
    "# Make a prediction using the model\n",
    "results = model.predict(list_of_images)\n",
    "\n",
    "# Since we are only testing one image, we only need to check the first result\n",
    "single_result = results[0]\n",
    "\n",
    "# We will get a likelihood score for all 10 possible classes. Find out which class had the highest score.\n",
    "most_likely_class_index = int(np.argmax(single_result))\n",
    "class_likelihood = single_result[most_likely_class_index]\n",
    "\n",
    "# Get the name of the most likely class\n",
    "class_label = class_labels[most_likely_class_index]\n",
    "\n",
    "# Print the result\n",
    "print(\"This is image is a {} - Likelihood: {:2f}\".format(class_label, class_likelihood))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257be3e2",
   "metadata": {},
   "source": [
    "# 5. Fine-Tuning Pre-trained Neural Networks\n",
    "\n",
    "### Pre-trained neural networks included with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1024bcf2",
   "metadata": {},
   "source": [
    "* Researchers around the world compete to build the most accurate image recognition neural networks \n",
    "* **So instead of inventing our own neural network designs from scratch, it often makes sense to reuse an existing neural network design as a starting point for your own projects.**\n",
    "* Even better, researcher also trained these neural network designs on large datasets and share the *trained* versions of the neural networks. So we can take those pre-trained neural networks and either reuse them directly or use them as a starting point for our own training\n",
    "* Keras includes copies of many popular pre-trained NNs that are ready to use.\n",
    "* **The image recognition models included with Keras are all trained to recognize images from the ImageNet dataset.**\n",
    "* The **ImageNet** data set is a collection of millions of pictures of objects that have been labeled so that you can use them to train computers to recognize those objects\n",
    "    * Each year ImageNet holds a worldwide image recognition contest called the **ImageNet Large Scale Visual Recognition Challenge** or **ILSVC**\n",
    "    * Note that the pre-trained models included with Keras are trained on the more limited dataset used by this contest \n",
    "    * **This (limited) data set contains images of 1,000 differents types or classes of objects; the data set includes over 1200 pictures of just, for example, Granny Smith apples.**\n",
    "    \n",
    "#### Image Recognition Models\n",
    "* Let's talk about the NN designs included with Keras that we can reuse:\n",
    "    * **VGG** (University of Oxford)- DNN; Very standard CNN design; 16 or 19 layers; state of the art circa 2014; still used widely today as a basis for other models because its **easy to work with and understand,** but it's not as efficient as newer designs. \n",
    "    * **ResNet-50** (Microsoft Research) 50-layer NN; More complex design; state of the art circa 2015; more accurate than VGG but uses less memory; ResNet uses a more complicated design where higher layers in the NN are connected to not just the layer directly below them, but they also have multiple connections to deeper layers.\n",
    "    * **Inception v-3** (Google); another 2015 design; even more complex design than ResNet50; also performs very well; its layers branch out into multiple separate paths before rejoining\n",
    "    \n",
    "**These (above) networks show the research trends in 2014 and 2015 to make NNs bigger and more complex that try to increase their accuracy. More recent NN designs tend to be more specialized:**\n",
    "\n",
    "   * **MobileNet** (Google); 2017, low resource usage; specifically designed to be able to run well on low power devices\n",
    "   * **NASNet** (Google): end of 2017; explores the idea of having algorithms design NNs. In a sense they're using ML to build and tweak ML models on their own; this let them create something that was moe accurate than existing models while still using even less computer power\n",
    "   \n",
    "#### Two Uses\n",
    "* Having access to these pre-trained models is useful for two reasons:\n",
    "    * **1) Reuse a trained model directly in your own programs to recognize objects in images** (if you need to recognize any of the 1,000 types of objects they're already trained on.\n",
    "    * **2) $\\star$ Transfer learning: Adapt an existing model to recognize new types of objects instead of starting from scratch.** $\\star$\n",
    "    \n",
    "    \n",
    "### Using a pre-trained network for object recognition\n",
    "* All the pretrained models included with Keras are under the `applications` package\n",
    "    * `from keras.applications import vgg16`\n",
    "    \n",
    "```\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import vgg16\n",
    "\n",
    "# Load Keras' VGG16 model that was pre-trained against the ImageNet database\n",
    "model = vgg16.VGG16()\n",
    "\n",
    "# Load the image file, resizing it to 224x224 pixels (required by this model)\n",
    "img = image.load_img(\"bay.jpg\", target_size=(224, 224))\n",
    "\n",
    "# Convert the image to a numpy array\n",
    "x = image.img_to_array(img)\n",
    "\n",
    "# Add a fourth dimension (since Keras expects a list of images)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "# Normalize the input image's pixel values to the range used when training the neural network\n",
    "x = vgg16.preprocess_input(x)\n",
    "\n",
    "# Run the image through the deep neural network to make a prediction\n",
    "predictions = model.predict(x)\n",
    "\n",
    "# Look up the names of the predicted classes. Index zero is the results for the first image.\n",
    "predicted_classes = vgg16.decode_predictions(predictions)\n",
    "\n",
    "print(\"Top predictions for this image:\")\n",
    "\n",
    "for imagenet_id, name, likelihood in predicted_classes[0]:\n",
    "    print(\"Prediction: {} - {:2f}\".format(name, likelihood))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f7ab5f",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "* If you have a lot of training data, you can train a CNN to recognize objects and images, but here's a secret: **In the real world, you almost never need to train the neural network from scratch. Instead, we use *transfer learning*.**\n",
    "* **Transfer learning** is using a model trained on one set of data as a starting point for modeling a new set of data (to give it a head start of solving a problem.\n",
    "* A typical CNN is structured like this:\n",
    "\n",
    "<img src='data/nn13.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec215722",
   "metadata": {},
   "source": [
    "* The network is made up of a series of convolutional layers and the training process teaches each of those layers to be activated when it sees certain patterns in the input image. Those layers learn to tell images apart by looking for those unique patterns\n",
    "* Notice that as the layers get deeper, the patterns get more complex that the given layer is looking for:\n",
    "\n",
    "<img src='data/nn14.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "<img src='data/nn15.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "<img src='data/nn16.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "<img src='data/nn17.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "<img src='data/nn18.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a3edc7",
   "metadata": {},
   "source": [
    "* Since NNs are really just mathematical models, it's impossible to tell exactly what each of these patterns represents, but you can see how each layer is getting more and more complex in what it's looking for \n",
    "* The basic idea is that **neural networks learn to detect simple patterns in the top layer, and then the next layer uses that information to look for slightly more complex patterns and so on down through the convolutional layers.**\n",
    "* But the final layer of the neural network is a densely connected layer that uses the information from the convolutional layers to decide which object is in the image.\n",
    "* With transfer learning, we're gonna start with a NN that's already been trained to recognize objects from a large dataset like ImageNet. To reuse this NN with new data, we can simply **slice off the last layer** (the dense output layer). We'll **keep all the layers that detect patterns, but remove the part that maps those patterns to specific objects.**\n",
    "* **We'll call this pre-trained NN a feature extractor because we're using it to extract training features from images.**\n",
    "* Next, we crete a new neural network to replace the last layer in the original network. This is the only part that we'll have to train ourselves.\n",
    "\n",
    "### Training with Transfer Learning \n",
    "* When we build our new image recognition system, we'll pass our new training images through the \"feature extractor\" and save the results for each training image to a file; then we'll use those extracted features to train the new neural network.\n",
    "* Since we're using the feature extractor to recognize shapes and patterns, our neural network only has to learn to tell which patterns map to which objects.\n",
    "* Since this new neural network isn't doing much work, it can learn to do it with a small amount of training data.\n",
    "\n",
    "### Predicting with Transfer Learning\n",
    "* When we want to test the new image, we have to first pass it through the same feature extractor.\n",
    "* Then we can use those extracted features as input to our newly-trained neural network, which will give us the final prediction.\n",
    "\n",
    "#### When to use transfer learning\n",
    "* Transfer learning can cut the time required to build an image recognition system from days to minutes \n",
    "* Always try it first-- because it's quick!\n",
    "* If transfer learning works, there's no need to do the extra work to train a new model\n",
    "* **Transfer learning is also really useful when you don't have a lot of training data but already have a model that solves a similar problem**.\n",
    "* \"Training a neural network from scratch is sort of like teaching a baby to read. The baby has to learn about letters and words and sentences before it can read and understand anything. Transfer learning is more like asking an adult that already knows how to read to learn something new.\"\n",
    "* If you oly have a few hundred training images for your image recognition system, you don't have enough data to teach your model from scratch so it makes sense to start with a model trained for something else and adapt it to your problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14347a5f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "050a9e75",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b1fc58b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ddbad2f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9af44af5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05079925",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19181b1a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a41dfe3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e00347ea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56731f3c",
   "metadata": {},
   "source": [
    "<img src='data/nn5.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
