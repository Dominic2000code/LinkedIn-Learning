{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b74cd599",
   "metadata": {},
   "source": [
    "# Deep Learning: Face Recognition\n",
    "#### Linked-In Learning with: Adam Geitgey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5798cef",
   "metadata": {},
   "source": [
    "#### Face Recognition now widely available\n",
    "* Researchers openly shared their solutions for building face recognition systems\n",
    "* But even if you know how to build a face recognition system, you still need a large dataset of images of people to train it.\n",
    "* Companies like Facebook and Google already have access to large data sets like this since they have millions of users uploading photos of themselves everyday\n",
    "* Researchers outside of these big companies have to work a little bit harder to build their own training data sets from images posted publicly online \n",
    "    * Anyone with access to a large training data set of images can build a face recognition system.\n",
    "* The end results is that face recognition is now available to almost anyone in all kinds of products\n",
    "\n",
    "#### What can you do with face recognition?\n",
    "* The simplest use of face recognition is to check whether a specific person you already know is present in an image\n",
    "    * This is called **identity verification**\n",
    "* To do this, you capture photographs of the person you want to recognize and you use those photographs to train the face recognition system\n",
    "    * The system can recognize that person when it sees them again\n",
    "    * **This can be used as an alternative to normal user login systems or key card entrance systems.**\n",
    "    \n",
    "***\n",
    "* Face recognition can also be used to quickly sort through large collections of images.\n",
    "* Face recognition has many applications in security and surveillance\n",
    "    * Imagine that you work for the police; you could feed a picture of a person of interest and the system search raw surveillance footage and pull out any video clips where that suspect appears; you could even do this across hundreds of cameras; that allows you to track a person as they move between cameras automatically\n",
    "* Besides recognizing specific people that you know, face recognition can also be used to tell you when a new person appears\n",
    "    * Count the number of unique people who appear in a video feed\n",
    "    * **Some electronic billboards now contain cameras and use this technique to count the number of unique people that stop and look at the sign.** This info is used to measure the effectiveness of the advertisement\n",
    "* Because face recognition systems are based on the model of how faces appear, they can be used to measure how similar (or not) two different people look\n",
    "    * \"Celebrity doppelganger\"\n",
    "  \n",
    "#### 5 uses of face recognition\n",
    "* Identity verification\n",
    "* Automatically organizing raw photo libraries by person\n",
    "* Tracking a specific person\n",
    "* Counting unique people\n",
    "* Finding people with similar appearances\n",
    "\n",
    "### Tools for Face Recognition\n",
    "   \n",
    "#### Commerical Face Recognition Services\n",
    "* Several large vendors provide face recognition APIs that you can use over the internet for a small fee\n",
    "* They all work in similar ways and provide similar features but they are **trained with different data sets, meaning that their accuracy may be better or worse depending on your specific application.**\n",
    "* **Some commonly used services are:**\n",
    "    * **Amazon Rekognition API**\n",
    "        * Face recognition, emotion detection, and motion tracking\n",
    "    * **Microsoft Azure Face API**\n",
    "        * Face recognition, age and gender detection, and face similarity matching \n",
    "* Both require an internet connection to use and you have to pay a small fee each time you use them\n",
    "* Also, using those services requires uploading all your face data to a third-party, since the face recognition happens on their computers in the cloud\n",
    "* For some applications, it's better to be able to do face reconition without a data connection and without having to share your data with anyone\n",
    "    * In these cases, open-source face recognition systems might be a better choice\n",
    "    \n",
    "#### Open-Source Face Recognition Tools\n",
    "* Can run locally on your computer without any external connections and without sharing any data\n",
    "* **Two of the most popular are:**\n",
    "    * **OpenFace:**\n",
    "        * Created by Brandon Amos and Carnegie Mellon University\n",
    "    * **dlib:**\n",
    "        * Created by Davis King\n",
    "        * A general purpose ML and computer vision library that has lots of features \n",
    "        * The instructor of this course, **Adam Geitgey** has written a Python library called **Face-Recognition** that makes it easier to use **dlib**\n",
    "* Both are free and open-source  \n",
    "\n",
    "## Face Recognition as a multi-step pipeline\n",
    "* Recognizing a face is a complicated problem with several steps \n",
    "    * We have to chain together several different ML algorithms into a pipeline to complete the entire task of face recognition\n",
    "    \n",
    "### Step 1: Locate and Extract Faces\n",
    "* Locate faces within the larger image\n",
    "* Once we know where the face is located in the image, we'll extract that area as a new image\n",
    "* This is the only part of the image that will pass to the next step of our face recognition pipeline (face detected within square).\n",
    "\n",
    "### Step 2: Identify Facial Features\n",
    "* Now we have a face image to work with, but we can't compare it directly to other face images \n",
    "* To be able to compare this face image with other faces, we need to be able to understand **how the person's head was turned or posed when the photo was taken;** a person's face looks different from different angles. If we don't take the person's head position into account, our face recognition system will think that the same person is two different people just because the person's head was turned a different way\n",
    "\n",
    "\n",
    "<img src='data/identify_facial_features.png' width=\"200\" height=\"100\" align=\"center\"/>\n",
    "\n",
    "* To do this, we'll **use a machine learning algorithm that can look at a single face and identify the location of each facial feature within the face.**\n",
    "    * We'll look for the position of the:\n",
    "        * Eyes\n",
    "        * Nose\n",
    "        * Mouth\n",
    "* We pass the face image and the location of each facial feature to the next step of our face recognition pipeline\n",
    "\n",
    "### Step 3: Align Faces Using Pose\n",
    "* The next step is to try to correct for the position or pose of the person's head\n",
    "* We know the position of each facial feature; **each person's face is unique, but we can assume that all faces follow roughly the same structure.**\n",
    "\n",
    "\n",
    "<img src='data/align_with_pose.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "* On the right is a face position template. This template shows the average position of facial features across lost of people, assuming that the person is looking directly forward\n",
    "* By comparing the position of each point on the left with the position of each point on the template, we can guess how far the head is turned and in what direction it was turned\n",
    "* Then, we can warp our face image to roughly match the template\n",
    "* This is called **aligning the face** because we are making sure the key facial features in the image line up with the face template before we move on to the next step in our pipeline\n",
    "\n",
    "### Step 4: Represent Face as Measurements\n",
    "* Now that we have aligned the face image, we're ready to turn the face into a set of numbers or measurements that represent this unique face.\n",
    "* Other pictures of the same person should generate measurements that are very close to these numbers\n",
    "* We'll use a neural network that was trained on millions of faces to come up with its own way to measure faces\n",
    "\n",
    "### Step 5: Compare to Other Faces\n",
    "* Now we can compare to other images by processing them the same way.\n",
    "* To compare two faces, we'll calculate how different the measurements are using a formula\n",
    "* **Euclidean distance between faces:**\n",
    "    * $d(face_1, face_2) = 0.2304$\n",
    "    * This formula basicaly measures how far apart the two different sets of measurements are \n",
    "        * If the measurements are close, we'll call it a match\n",
    "        \n",
    "        \n",
    "#### Face Recognition Pipeline Steps\n",
    "* Step 1: Locate and extract faces from each image\n",
    "* Step 2: Identify facial features in each image\n",
    "* Step 3: Align faces to match pose template\n",
    "* Step 4: Encode faces using a trained neural network (aka **Face Encoding**)\n",
    "* Step 5: Check Euclidean distance between face encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7345622b",
   "metadata": {},
   "source": [
    "# Unit 3: Face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95aad4d",
   "metadata": {},
   "source": [
    "#### What is face detection?\n",
    "* **Face detection** is the ability to detect and locate human faces in a photograph\n",
    "* We use face detection here to extract each face from a photograph and pass it to the next step in our face recognition pipeline\n",
    "\n",
    "### Step 1: Sliding Window Classifier\n",
    "* The easiset way to locate objects in an image is to build a **sliding window classifier.**\n",
    "* Two steps:\n",
    "    * 1) Build a simple face detection model using an ML model \n",
    "    * 2) Slide the simple face detector across a larger image \n",
    "* When a face is detected, we record the location of the face\n",
    "#### Face detection algorithms we can use for sliding window classifier:\n",
    "* Three of the most common are:\n",
    "    * **Viola-Jones**\n",
    "        * Uses decision trees to detect faces based on light and dark areas\n",
    "        * Developed early 2000s\n",
    "        * Pro: Very fast and great for low-powered devices\n",
    "        * Con: Not very accurate; tends to have a lot of FPs (false positives)\n",
    "        * No real reason to use this anymore unless you're working with very low-powered devices\n",
    "    * **Histogram of oriented gradients (HOG)**\n",
    "        * Invented 2005\n",
    "        * Looks for shifts from light to dark areas in an image\n",
    "        * Slower than Viola-Jones, but more accurate\n",
    "        * Runs well on normal computers without special hardware\n",
    "        * **This is the algorithm we'll use to build FRS in this course.**\n",
    "    * **CNNs**\n",
    "        * Uses a deep neural network to detect faces\n",
    "        * Very accurate (**MOST accurate**), but **requires a lot of training data.**\n",
    "        * Runs best on computers with dedicated GPUs\n",
    "        * It will run very slowly otherwise: need to have right hardware\n",
    "* Important to remember that face detection is its own separate step in our face recognition pipeline\n",
    "\n",
    "### Analyzing an Image as a Histogram of Oriented Gradients (HOG)\n",
    "#### Step 1: Convert to black and white\n",
    "* HOG algorithm only looks at differences between light and dark areas in an image; it doesn't need color information\n",
    "\n",
    "<img src='data/hog1.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "* Our goal is to measure how dark a given pixel is in comparison to the pixels surrounding it and **find the direction where the biggest change happens**\n",
    "* In this case, we can see that the pixel to the left of the center pixel is much lighter than the center pixel, and the pixel to the right is darker than this pixel\n",
    "* In other words, at this exact point, the image is transitioning from a light area to a darker area\n",
    "* Based on that, we draw an arrow on top of this pixel that points from left to right\n",
    "* **This shows the movement of lighting at this exact point.**\n",
    "\n",
    "<img src='data/hog2.png' width=\"350\" height=\"125\" align=\"center\"/>\n",
    "\n",
    "* If we repeat this process for every single pixel in the image, **the image turns into a map of transitions from light to dark areas.**\n",
    "* These lines are called **gradients.**\n",
    "\n",
    "<img src='data/hog3.png' width=\"350\" height=\"125\" align=\"center\"/>\n",
    "\n",
    "* Each gradient shows how the image flows from a light area to a dark area at that point.hog5.png\n",
    "* Now let's zoom back out and see what the gradient map looks like:\n",
    "\n",
    "<img src='data/hog4.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "<img src='data/hog5.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "* The **gradient map** is a simplified version of the original image, but it's still pretty complex\n",
    "* Capturing the gradient for every single pixel is more detail than we need\n",
    "* To detect faces, all we really need is to detect the overall structure of the image\n",
    "* In other words, we can simplify this representation further\n",
    "* Instead of keeping track of each separate gradient within the image, we'll just store a count of how many gradient points in each direction\n",
    "* The original image is now a simple representation that captures the basic structure:\n",
    "\n",
    "<img src='data/hog6.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "* We can use this simplified representation to easily train a face detection model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6e36ba",
   "metadata": {},
   "source": [
    "### Finding faces in images with HOG features \n",
    "#### Step 1: Collecting training data\n",
    "* Convert data to HOG representations\n",
    "#### Step 2: Train face classifier on HOG faces\n",
    "* HOG face detectors can perform well with a fairly small amount of training data\n",
    "#### Step 3: Sliding window classifier on HOG\n",
    "* Any part of the image that returns true is a part of the image that contains a face\n",
    "* HOG is a simplified representation of an image that still captures enough detail to detect faces \n",
    "* A HOG representation is not affected by small changes in lighting\n",
    "* A HOG representation is not affected by small changes in an object's shape\n",
    "\n",
    "### Coding face detection\n",
    "* We'll be using a pre-trained HOG face detector to detect all the faces that appear in an image\n",
    "* Because most human faces have roughly the same structure, the pretrained face detection model will work well for almost any image\n",
    "* **There's no need to train a new one from scratch**\n",
    "* **PIL** is the **Python Image Library**: it lets us easily display an image on the screen, and draw lines on top of the image\n",
    "### `face_recognition` is the library that gives us access to the face detection model in `dlib`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addefb56",
   "metadata": {},
   "source": [
    "* **Note:** that the `PIL` library works with images in its own internal format\n",
    "    * So, we need to convert the image array into a `PIL` formatted image\n",
    "    \n",
    "# Chapter 4: Facial Feature Detection\n",
    "### What is face landmark estimation?\n",
    "* **Face Landmark Estimation:** Finding the location of key points on a face, such as the tip of the nose and center of each eye.\n",
    "    * Works by starting with a known set of points that should appear on any face\n",
    "    * It then moves those point around until they match the face image\n",
    "    * The (orange) landmark model shown above is called a **68 point face landmark model**\n",
    "    * To make face recognition systems run a little more quickly, we can also use a face landmark model with fewer points (like a **5 point model**, which only detects the edges of each eye and the bottom of the nose)\n",
    "    \n",
    "* Social media lenses: Adding snap filters or make-up to your face: uses face landmark models\n",
    "* These applications work by first detecting the face landmarks and then using those points to overlay clothes or makeup in the right place\n",
    "* The main use for face landmark estimation is: **Face Alignment**, where we correct for head rotation when doing face recognition\n",
    "\n",
    "### Identifying Face Landmarks with an ML Model\n",
    "* **Trick 1:** Assume all human faces are similar and roughly the same shape\n",
    "    * Overlay the entire face template on the face, and then we'll only ask the computer to move and adjust the template so that each point is closer to the right point \n",
    "    \n",
    "* **Trick 2:** Limit Movement of Each Point\n",
    "    * Add the constraint of how much the computer can move each point\n",
    "    * The rule is that no single point can be moved too far from its neighboring points\n",
    "    * Notice that the below landmark points move a little from the original template, but no point moves too far from its neighboring points\n",
    "    \n",
    "<img src='data/landmarks1.png' width=\"300\" height=\"150\" align=\"center\"/>\n",
    "\n",
    "* **Trick 3:** Fine tune with multiple models:\n",
    "    * Split the job of completely fitting the face template into the face into smaller problems\n",
    "    * In other words, train several different ML models that each do part of the job\n",
    "    * Subsequent models only need to learn to fix the mistakes of past models, making each of their jobs easier (because no one model needs to learn the entire process of fitting the template to the face, they just need to learn to make improvements)\n",
    "    * This process continues with as many as 10 models\n",
    "    \n",
    "#### Automatic Face Landmark Estimation\n",
    "* Once this cascade to face landmark model is trained, it should work for pretty much any face\n",
    "* So, in our code, we won't have to train our own model from scratch\n",
    "* We can just use a standard pre-trained model that should work for all of our images\n",
    "* **Understanding how the model works allows you to understand its limitations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17adbee",
   "metadata": {},
   "source": [
    "### Posing faces based on face landmarks\n",
    "* **Face alignment:** is where we adjust each face image so that key facial features (like the eyes, nose, and mouth) line up with a predefined template\n",
    "* Correcting for head angle and rotation will make our face recognition system more accurate\n",
    "\n",
    "#### Steps of face alignment\n",
    "* Detect face landmarks\n",
    "* Calculate affine transformation\n",
    "    * **Affine transformatio:** A linear mapping between sets of points where parallel lines will remain parallel\n",
    "    * Basically, we can move, rotate, and stretch our image, but we can't do more complex things like twisting or warping\n",
    "    * We don't have to write any code to do this alignment, the face_recognition library will do this for us\n",
    "* Each face in the list generated by `.face_landmarks` will be a python dictionary object where the keys are the names of the facial feature (\"left eye\", \"right eye\", \"chin\", etc) and the values are the list of `(x,y)` coordinates of the points that correspond to that facial feature\n",
    "\n",
    "### Representing a face as a set of measurements\n",
    "* The most important step in our face recognition pipeline: telling faces apart from each other\n",
    "* The problem: We have a set of known faces in a database and an unknown face we'd like to identify\n",
    "* The simplest approach is to take the unknown face and compare it to the known faces one by one\n",
    "\n",
    "<img src='data/face_comp.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "#### Comparing images doesn't work \n",
    "* Too slow (the bigger the database the slower)\n",
    "* Doesn't capture the structure of each face\n",
    "    * Different positions will throw off the comparison\n",
    "    * Different backgrounds and clothing and hairstyles will throw off the model\n",
    "    \n",
    "* Solution: Representing faces as measurements\n",
    "<img src='data/face_meas.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c95008",
   "metadata": {},
   "source": [
    "<img src='data/face_meas2.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "<img src='data/face_meas3.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c0f015",
   "metadata": {},
   "source": [
    "* **Face encoding:** The process of taking an image of a face and turning it into a set of measurements\n",
    "* A real face-encoding system will capture a large number of face measurements (typically 128 or more)\n",
    "* Instead of trying to decide on 128 ways to measure a face, we'll use ML to create those measurements\n",
    "* **Deep Metric Learning:** Using deep learning to have a computer come up with a way to measure something that you don't know how to measure yourself\n",
    "* **Training Triplets**: 2 different pictures of the same person, and one picture of a different person: goal is for computer to find a set of measurements that keeps the measurements of the two pictures of the same person closer than either of their measurements with the picture of someone else\n",
    "* **Because a trained model should for any picture of any person, that means that we only have to train the face encoding model once.**\n",
    "* When we write the code for our face recognition system, we'll be using a pre-trained model instead of training one from scratch. In most cases, this will work fine and you won't ever need to retrain your own face encoding model \n",
    "* **Model interpretability is a common problem in machine learning**\n",
    "* **When models are hard to interpret, they can often have hidden biases. Watch out for this!**\n",
    "* **In the face recognition model, there's often a hidden bias for the model to be more accurate for people from one region of the world than another.**\n",
    "* **Face distance threshold:** set a face maximum distance that is still considered the same face. For our exercises we use 0.6\n",
    "\n",
    "#### Advantages of Using Euclidean Distance\n",
    "* Fast to calculate and easy to parallelize\n",
    "* Works nicely with other common ML algorithms like KNN\n",
    "* Makes it easy to store and query measurements using a standard database\n",
    "\n",
    "* For faces of known people, we want to make sure there is only one person in the picture, that they are facing the camera, and that they are clear and visible and reasonable lighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe17cbca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda82098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a914e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc05d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f65c71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb2ff7ff",
   "metadata": {},
   "source": [
    "<img src='data/hog6.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
